{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5fd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, NeighborSampler, Batch, DataLoader\n",
    "from torch_geometric.nn import SAGEConv, GAE, TopKPooling\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import Embedding\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader as api\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c4e0f62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/prepro_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ab8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     548804\n",
      "8      76971\n",
      "10     60024\n",
      "7      55852\n",
      "9      50494\n",
      "5      38416\n",
      "6      26670\n",
      "4       6462\n",
      "3       4374\n",
      "2       2019\n",
      "1       1307\n",
      "Name: Book-Rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 'Book-Rating' 컬럼 값의 빈도수 계산\n",
    "rating_counts = dataset['Book-Rating'].value_counts()\n",
    "\n",
    "# 결과 출력\n",
    "print(rating_counts)\n",
    "\n",
    "# 심각한 class imbalance 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5868c3be",
   "metadata": {},
   "source": [
    "# Make Graph Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4f503",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a59f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(dataset['Age'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a013f15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_tensor.shape >>>  torch.Size([871393, 1])\n",
      "tensor([[0.0943],\n",
      "        [0.0943],\n",
      "        [0.0943],\n",
      "        ...,\n",
      "        [0.1844],\n",
      "        [0.1762],\n",
      "        [0.1434]])\n"
     ]
    }
   ],
   "source": [
    "print(\"age_tensor.shape >>> \",age_tensor.shape)\n",
    "print(age_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477d97a",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20483b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Location\n",
    "\n",
    "# LabelEncoder를 사용해 위치 정보를 정수로 변환\n",
    "le = LabelEncoder()\n",
    "dataset['Location_encoded'] = le.fit_transform(dataset['Location'])\n",
    "\n",
    "# 임베딩 레이어 초기화\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=29)\n",
    "\n",
    "# 위치 정보를 10차원 벡터로 변환\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "\n",
    "# 결과를 NumPy array로 변환\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53bddc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_embeddings.shape >>>  (871393, 29)\n",
      "[[ 1.6865574   0.54743063 -2.0043907  ...  0.33478603  1.4337459\n",
      "  -0.8887633 ]\n",
      " [ 1.6865574   0.54743063 -2.0043907  ...  0.33478603  1.4337459\n",
      "  -0.8887633 ]\n",
      " [ 1.6865574   0.54743063 -2.0043907  ...  0.33478603  1.4337459\n",
      "  -0.8887633 ]\n",
      " ...\n",
      " [ 1.6865574   0.54743063 -2.0043907  ...  0.33478603  1.4337459\n",
      "  -0.8887633 ]\n",
      " [ 0.26706204 -0.5974826  -0.37141737 ...  0.14246517 -0.56659603\n",
      "  -0.40958616]\n",
      " [-1.9786525   0.64788187 -0.6809563  ... -0.3133516  -0.25659916\n",
      "  -0.7243495 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"location_embeddings.shape >>> \",location_embeddings.shape)\n",
    "print(location_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e452e2",
   "metadata": {},
   "source": [
    "### Book-Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1c0cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# # 사전 훈련된 FastText 모델 다운로드\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "# !gunzip cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30341854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# FastText 모델 로드\n",
    "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ff05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_embedding_fasttext(title):\n",
    "    if not isinstance(title, str):\n",
    "        title = \"\"\n",
    "    words = title.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    embeddings = [fasttext_model.get_word_vector(word) for word in words]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aeea42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = dataset['Book-Title'].apply(get_title_embedding_fasttext).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d92134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈 문자열이거나 토큰화된 단어가 없는 책 제목의 개수: 0\n"
     ]
    }
   ],
   "source": [
    "empty_titles_count = 0\n",
    "for title in dataset['Book-Title']:\n",
    "    words = title.split()\n",
    "    if not words:\n",
    "        empty_titles_count += 1\n",
    "\n",
    "print(f\"빈 문자열이거나 토큰화된 단어가 없는 책 제목의 개수: {empty_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8953c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩이 numpy 배열이 아니거나 형태가 (300,)이 아닌 경우 해당 인덱스, 책 제목, 및 임베딩을 출력\n",
    "\n",
    "for idx, emb in enumerate(title_embeddings):\n",
    "    if not isinstance(emb, np.ndarray) or emb.shape != (300,):\n",
    "        print(f\"Index: {idx}, Title: {dataset['Book-Title'][idx]}, Embedding: {emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1092cb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA 객체를 생성\n",
    "pca = PCA(n_components=20)\n",
    "\n",
    "# 평균 임베딩 벡터로 구성된 리스트를 NumPy 배열로 변환\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "\n",
    "# 차원 축소\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fc423d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduced_title_embeddings.shape >>>  (871393, 20)\n",
      "[[-0.1840866   0.07394366  0.31140894 ...  0.03352589  0.09178416\n",
      "  -0.05522412]\n",
      " [-0.03533659 -0.05473807 -0.11056273 ...  0.01496605  0.00451254\n",
      "  -0.07433714]\n",
      " [-0.06374066 -0.06558958 -0.15763324 ...  0.00198658  0.00080501\n",
      "  -0.03975548]\n",
      " ...\n",
      " [ 0.32297003  0.44601208 -0.15708432 ... -0.04339454  0.1211694\n",
      "   0.01938819]\n",
      " [ 0.01626778 -0.04580672 -0.03094558 ... -0.05879885  0.10792992\n",
      "  -0.00517468]\n",
      " [ 0.01755417 -0.12413795 -0.08622356 ... -0.09719356  0.03359084\n",
      "   0.01332647]]\n"
     ]
    }
   ],
   "source": [
    "print(\"reduced_title_embeddings.shape >>> \",reduced_title_embeddings.shape)\n",
    "print(reduced_title_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f3522f",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98a0790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Publisher\n",
    "# LabelEncoder를 사용해 위치 정보를 정수로 변환\n",
    "le = LabelEncoder()\n",
    "dataset['Publisher_encoded'] = le.fit_transform(dataset['Publisher'])\n",
    "\n",
    "# 임베딩 레이어 초기화\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=10)\n",
    "\n",
    "# 위치 정보를 10차원 벡터로 변환\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "\n",
    "# 결과를 NumPy array로 변환\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee4e7045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publisher_embeddings.shape >>>  (871393, 10)\n",
      "[[ 0.76888365  1.9811172  -0.35688967 ... -1.0785253   0.72855276\n",
      "  -0.33809376]\n",
      " [-2.3139355   0.7099615   0.8230311  ... -0.8418956  -0.2198488\n",
      "  -0.024038  ]\n",
      " [-2.3139355   0.7099615   0.8230311  ... -0.8418956  -0.2198488\n",
      "  -0.024038  ]\n",
      " ...\n",
      " [ 0.5427154   0.03457269 -0.8253357  ...  0.25098833 -1.0882536\n",
      "  -0.28632239]\n",
      " [ 0.8391219  -0.2962211   0.19064201 ...  0.30438435  1.3664846\n",
      "   0.74028295]\n",
      " [ 0.6650448   0.38826466  0.79314446 ... -0.6144511   1.4150337\n",
      "   0.38412946]]\n"
     ]
    }
   ],
   "source": [
    "print(\"publisher_embeddings.shape >>> \",publisher_embeddings.shape)\n",
    "print(publisher_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b3497a",
   "metadata": {},
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "<이슈 발견>  \n",
    "아니? 근데 unique한 책 제목 개수는 217829개야. 책 ID가 25612개 더 많아.      \n",
    "그렇다는 건 같은 책 제목에 다른 책 아이디가 배정되었단 거네?    \n",
    "고민 사항.\n",
    "번역본을 각각 다른 책으로 취급하는 것이 모델 성능에 도움이 될 수 있다.    \n",
    "하지만, 모델이 같은 책 제목에 대해 여러 번의 추천될 수 있다.\n",
    "> 이 부분은 테스트해봐야 하는 부분 같다.  우선은 통합 없이 그냥 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2538530",
   "metadata": {},
   "outputs": [],
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "# User-ID 열의 unique한 값들을 리스트로 만들기  > 83256\n",
    "unique_user_ids = dataset['User-ID'].unique().tolist()\n",
    "# Book-ID 열의 unique한 값들을 리스트로 만들기 > 243441\n",
    "unique_book_ids = dataset['Book-ID'].unique().tolist()\n",
    "\n",
    "# unique_user_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_user_ids_df = unique_user_ids_df.sort_values(by='User-ID', ascending=True)\n",
    "# # 인덱스를 새 column으로 추가\n",
    "sorted_unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_user_ids_df.rename(columns={'index': 'UserNodeID'}, inplace=True)\n",
    "\n",
    "# unique_book_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_book_ids_df = unique_book_ids_df.sort_values(by='Book-ID', ascending=True)\n",
    "# #인덱스를 새 column으로 추가\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_book_ids_df.rename(columns={'index': 'BookNodeID'}, inplace=True)\n",
    "\n",
    "# 인덱스 시작 번호를 83256으로 설정\n",
    "sorted_unique_book_ids_df['BookNodeID'] += 83256\n",
    "\n",
    "# UserNodeID  0 ~ 83255\n",
    "# BookNodeID   83256 ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db6e68a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ID_dict(df):\n",
    "    ID_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        ID_dict[row[1]] = row[0]\n",
    "    return ID_dict\n",
    "\n",
    "UserNodeID_dict = make_ID_dict(sorted_unique_user_ids_df)\n",
    "BookNodeID_dict = make_ID_dict(sorted_unique_book_ids_df)\n",
    "\n",
    "####### 매핑 진행\n",
    "dataset['User-ID'] = dataset['User-ID'].map(UserNodeID_dict)\n",
    "dataset['Book-ID'] = dataset['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b9984",
   "metadata": {},
   "source": [
    "## node feature\n",
    "\n",
    "node feature 데이터프레임을 만들고, 노드 ID(인덱스)를 통해 feature 값 붙이기.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509d94d",
   "metadata": {},
   "source": [
    "node feature matrix 구성:\n",
    "1. 데이터 전처리를 통해, 원본 데이터셋의 user id와 book id를 겹치지 않는 숫자로 매핑\n",
    "2. UserNodeID는  0에서부터 83255, 그리고 BookNodeID   는 83256부터 326696의 정수로 할당\n",
    "3. 각각 생성한 embedding들을 이어붙인 다음, user_features 와 book_features를 위아래로 이어붙어 node feature matrix 인 x를 생성. x의 shape는 [326697, 30].\n",
    "4. 다시 말해, edge_list에서 노드 인덱스를 사용하여 node feature를 쉽게 가져오도록 위처럼 구성하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec0979cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_nodes = len(dataset['User-ID'].unique().tolist())\n",
    "num_book_nodes = len(dataset['Book-ID'].unique().tolist())\n",
    "\n",
    "user_ids = dataset['User-ID'].unique().tolist()  # 유니크 처리함\n",
    "book_ids = dataset['Book-ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "270d00ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = max(user_ids) + 1\n",
    "num_books = max(book_ids) - num_user_nodes + 1\n",
    "\n",
    "feature_dim = 30  # age (1) + location (29)  # title (20) + publisher (10)\n",
    "\n",
    "user_features = np.zeros((num_user_nodes, feature_dim))\n",
    "book_features = np.zeros((num_book_nodes, feature_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71db4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "# 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 스케일링을 개별적으로 수행\n",
    "age_tensor_scaled = scaler.fit_transform(age_tensor.reshape(-1, 1))\n",
    "location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor_scaled, location_embeddings_scaled):\n",
    "    user_features[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "title_embeddings_scaled = scaler.fit_transform(reduced_title_embeddings)\n",
    "publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, title_embeddings_scaled, publisher_embeddings_scaled):\n",
    "    book_features[book_id - num_user_nodes] = np.concatenate([title, publisher], axis=0)\n",
    "    \n",
    "# 사용자 노드 행렬과 도서 노드 행렬을 세로로 연결하여 최종 node_features 행렬 생성:    \n",
    "node_features = np.vstack((user_features, book_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd428d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_matrix = torch.tensor(node_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e58eeb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11.3436)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feature_matrix.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c042aad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.5275)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_feature_matrix.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773b492",
   "metadata": {},
   "source": [
    "# Graph Split - Node base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4107b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 노드의 개수를 계산\n",
    "num_nodes = len(node_features)\n",
    "\n",
    "# 데이터 분할을 위한 나눔\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# train 셋에만 오버샘플링 적용\n",
    "from sklearn.utils import resample\n",
    "n_samples = 60000\n",
    "df_list = [train_data[train_data['Book-Rating'] == i] for i in range(11)]\n",
    "resampled_df_list = [resample(df, replace=True, n_samples=n_samples, random_state=42) for df in df_list]\n",
    "resampled_train_data = pd.concat(resampled_df_list)\n",
    "\n",
    "# 각각의 데이터프레임에서 유니크한 값을 찾아냄\n",
    "train_user_ids = np.unique(resampled_train_data['User-ID'].values)\n",
    "train_book_ids = np.unique(resampled_train_data['Book-ID'].values)\n",
    "test_user_ids = np.unique(test_data['User-ID'].values)\n",
    "test_book_ids = np.unique(test_data['Book-ID'].values)\n",
    "\n",
    "#유니크한 값들을 이어붙임\n",
    "train_node_ids = np.concatenate((train_user_ids, train_book_ids))\n",
    "test_node_ids = np.concatenate((test_user_ids, test_book_ids))\n",
    "\n",
    "# 텐서로 만듦\n",
    "train_idx = torch.tensor(train_node_ids, dtype=torch.long)\n",
    "test_idx = torch.tensor(test_node_ids, dtype=torch.long)\n",
    "\n",
    "# 마스크 생성. 처음에는 모든 값이 False로 초기화되어 있음\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "#마스크 적용. 각 마스크에 대해 앞서 추출한 노드 ID에 해당하는 인덱스를 True로 설정.\n",
    "# 이렇게 하면 각 마스크는 해당 데이터셋에 속하는 노드를 나타내게 된다.\n",
    "train_mask[train_node_ids] = True\n",
    "test_mask[test_node_ids] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bcf97",
   "metadata": {},
   "source": [
    "## Weight & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c37734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = torch.tensor(dataset['Book-Rating'].values, dtype=torch.float).unsqueeze(-1)\n",
    "y = edge_attr.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc580461",
   "metadata": {},
   "source": [
    "## edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6025b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ratings = pd.concat([resampled_train_data, test_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54f0cc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 38697,  63170,  28355,  ...,  78966,  49967,  40093],\n",
       "        [267484, 155489, 176392,  ...,  83929, 257296, 269014]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = torch.tensor(combined_ratings[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()\n",
    "edge_index # torch.Size([2, 871393])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb485175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     109709\n",
      "8      15240\n",
      "10     12126\n",
      "7      11206\n",
      "9      10076\n",
      "5       7686\n",
      "6       5417\n",
      "4       1291\n",
      "3        877\n",
      "2        395\n",
      "1        256\n",
      "Name: Book-Rating, dtype: int64\n",
      "0     60000\n",
      "1     60000\n",
      "2     60000\n",
      "3     60000\n",
      "4     60000\n",
      "5     60000\n",
      "6     60000\n",
      "7     60000\n",
      "8     60000\n",
      "9     60000\n",
      "10    60000\n",
      "Name: Book-Rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 분할된 후에 다시 클래스 분포 확인해 보기. \n",
    "\n",
    "test_data_rating_counts = test_data['Book-Rating'].value_counts()\n",
    "print(test_data_rating_counts)\n",
    "\n",
    "resampled_train_data_rating_counts = resampled_train_data['Book-Rating'].value_counts()\n",
    "print(resampled_train_data_rating_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91887d64",
   "metadata": {},
   "source": [
    "## bulid Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "380b1f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68e73738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 데이터 생성\n",
    "data = Data(x=node_feature_matrix,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=y,\n",
    "            train_mask=train_mask,\n",
    "            test_mask=test_mask)\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22088312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conceptelling/miniconda3/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.NeighborSampler' is deprecated, use 'loader.NeighborSampler' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# 샘플러에 적용\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[5, 3], batch_size=32, shuffle=True, num_nodes=data.num_nodes)\n",
    "test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[5, 3], batch_size=32, shuffle=False, num_nodes=data.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eecd7ed",
   "metadata": {},
   "source": [
    "# GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f608ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class WeightedSAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(WeightedSAGEConv, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        row, col = edge_index\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = edge_attr[col]  # 인접 노드에 대한 edge_attr만 선택\n",
    "        edge_index, edge_attr = add_self_loops(edge_index, edge_attr, num_nodes=x.size(0))\n",
    "        x = self.lin(x)\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        if edge_attr is not None:\n",
    "            return x_j * edge_attr.view(-1, 1)\n",
    "        else:\n",
    "            return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da709732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGERegressor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout, activation_name, num_layers=2):\n",
    "        super(GraphSAGERegressor, self).__init__()\n",
    "\n",
    "        activations = nn.ModuleDict([\n",
    "            ['ReLU', nn.ReLU()],\n",
    "            ['ReLU6', nn.ReLU6()],\n",
    "            ['LeakyReLU', nn.LeakyReLU()],\n",
    "            ['PReLU', nn.PReLU()],\n",
    "            ['ELU', nn.ELU()],\n",
    "            ['SiLU', nn.SiLU()]\n",
    "        ])\n",
    "        \n",
    "        self.activation = activations[activation_name]\n",
    "        if self.activation is None:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_name}\")\n",
    "\n",
    "        self.user_convs = nn.ModuleList()\n",
    "        self.book_convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "\n",
    "        self.user_convs.append(WeightedSAGEConv(in_channels, hidden_channels))\n",
    "        self.book_convs.append(WeightedSAGEConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.user_convs.append(WeightedSAGEConv(hidden_channels, hidden_channels))\n",
    "            self.book_convs.append(WeightedSAGEConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.user_convs.append(WeightedSAGEConv(hidden_channels, out_channels))\n",
    "        self.book_convs.append(WeightedSAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index_list, edge_attr_list=None):\n",
    "        if edge_attr_list is None:\n",
    "            edge_attr_list = [None] * len(edge_index_list)\n",
    "\n",
    "        user_x = x\n",
    "        book_x = x\n",
    "\n",
    "        for i in range(len(self.user_convs) - 1):\n",
    "            user_x = self.user_convs[i](user_x, edge_index_list[i], edge_attr_list[i])\n",
    "            user_x = self.batch_norms[i](user_x)\n",
    "            user_x = self.activation(user_x)\n",
    "            user_x = self.dropout(user_x)\n",
    "\n",
    "            book_x = self.book_convs[i](book_x, edge_index_list[i], edge_attr_list[i])\n",
    "            book_x = self.batch_norms[i](book_x)\n",
    "            book_x = self.activation(book_x)\n",
    "            book_x = self.dropout(book_x)\n",
    "\n",
    "        user_emb = self.user_convs[-1](user_x, edge_index_list[-1], edge_attr_list[-1])\n",
    "        book_emb = self.book_convs[-1](book_x, edge_index_list[-1], edge_attr_list[-1])\n",
    "\n",
    "        return user_emb, book_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "845d8824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_channels = 30\n",
    "hidden_channels = 32\n",
    "out_channels = 1\n",
    "activation_name = 'ReLU'\n",
    "dropout =  0.1\n",
    "\n",
    "model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout, activation_name, num_layers=3)\n",
    "model = model.to(device)  # 모델만 옮기고, 데이터는 옮기지 않는다. 그래야 메모리 효율성 업!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e75d6",
   "metadata": {},
   "source": [
    "# Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed974831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 10 13:15:20 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.105.01   Driver Version: 515.105.01   CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   30C    P0    49W / 400W |   1323MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2557      C   ...ing/miniconda3/bin/python     1321MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f9c533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(train_loader, optimizer):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "#     for batch_size, n_id, adjs in train_loader:\n",
    "#         torch.cuda.empty_cache()\n",
    "#         adjs = [adj.to(device) for adj in adjs]  # Move adjs to device\n",
    "#         edge_index_list = [adj.edge_index.to(device) for adj in adjs]  # Move edge_index to device\n",
    "#         edge_attr_list = [data.edge_attr.to(device) for _ in adjs]  # Reuse edge_attr\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "#         user_emb, book_emb = out[:n_id.size(0)]  # Use n_id.size(0) instead of batch_size\n",
    "#         # Compute predicted ratings\n",
    "#         predictions = (user_emb[edge_index_list[0][0]] * book_emb[edge_index_list[0][1]]).sum(dim=-1)\n",
    "#         loss = criterion(predictions, data.y[edge_index_list[0][0]].squeeze().to(device))  # Use correct indexing for y\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "# def test(test_loader):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     for batch_size, n_id, adjs in test_loader:\n",
    "#         torch.cuda.empty_cache()\n",
    "#         adjs = [adj.to(device) for adj in adjs]  # Move adjs to device\n",
    "#         edge_index_list = [adj.edge_index.to(device) for adj in adjs]  # Move edge_index to device\n",
    "#         edge_attr_list = [data.edge_attr.to(device) for _ in adjs]  # Reuse edge_attr\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "#         user_emb, book_emb = out[:n_id.size(0)]  # Use n_id.size(0) instead of batch_size\n",
    "#         predictions = (user_emb[edge_index_list[0][0]] * book_emb[edge_index_list[0][1]]).sum(dim=-1)  # Compute predicted ratings\n",
    "#         loss = criterion(predictions, data.y[edge_index_list[0][0]].squeeze().to(device))  # Use correct indexing for y\n",
    "#         total_loss += loss.item()\n",
    "#     return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef47d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        edge_index_list = [adj.edge_index for adj in adjs]\n",
    "        edge_attr_list = [data.edge_attr for _ in adjs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        user_emb, book_emb = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "        pred = torch.sum(user_emb[edge_index_list[0][0]] * book_emb[edge_index_list[0][1]], dim=-1)\n",
    "        y = data.y[edge_index_list[0][0]].view(-1)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in test_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        adjs = [adj.to(device) for adj in adjs]\n",
    "        edge_index_list = [adj.edge_index for adj in adjs]\n",
    "        edge_attr_list = [data.edge_attr for _ in adjs]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_emb, book_emb = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "        pred = torch.sum(user_emb[edge_index_list[0][0]] * book_emb[edge_index_list[0][1]], dim=-1)\n",
    "        y = data.y[edge_index_list[0][0]].view(-1)\n",
    "        loss = F.mse_loss(pred, y)\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cd748e",
   "metadata": {},
   "source": [
    "# Beysian Hyper Parameter Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1629c64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 하이퍼파라미터 추천값 설정\n",
    "    in_channels = node_features.shape[1]\n",
    "    hidden_channels = trial.suggest_int('hidden_channels', 32, 128)\n",
    "    out_channels = 1\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    activation_name = trial.suggest_categorical('activation_name', ['ReLU', 'LeakyReLU', 'PReLU', 'ELU', 'SiLU'])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer_name', ['AdamW', 'Adam', 'RMSprop', 'Adagrad'])\n",
    "\n",
    "    #hidden_channels, dropout 등 숫자 값을 설정할 때, 가능한 경우 integer가 아닌 float으로 설정하는 것이 좋습니다.\n",
    "    # 이렇게 함으로써 float값 범위 내에서 모든 가능한 값을 테스트해볼 수 있게 됩니다.\n",
    "    \n",
    "    # 모델 및 최적화 생성\n",
    "    model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout, activation_name).to(device)\n",
    "\n",
    "    optimizer_class = getattr(torch.optim, optimizer_name)\n",
    "    optimizer_instance = optimizer_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #최적화 함수를 바꿀 때마다 optimizer_instance의 파라미터를 수정해줘야 하는데, 이 과정에서 누락될 가능성이 있습니다.\n",
    "    # 이를 방지하기 위해 optimizer_instance를 새로 생성해주는 것이 좋습니다.\n",
    "    \n",
    "    # DataLoader 수정\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[5,3], batch_size=batch_size, shuffle=True, num_nodes=data.num_nodes)\n",
    "    test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[5, 3], batch_size=batch_size, shuffle=False, num_nodes=data.num_nodes)\n",
    "\n",
    "    # 학습 및 평가 루프\n",
    "    best_test_loss = float('inf')\n",
    "    for epoch in range(1, 11):\n",
    "        train_loss = train(train_loader, optimizer_instance)\n",
    "        test_loss = test(test_loader)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_model_weights = deepcopy(model.state_dict())\n",
    "            trial.set_user_attr('best_model_weights', best_model_weights)\n",
    "            trial.set_user_attr('best_model', model)\n",
    "    tqdm.write(f'Trial {trial.number} - Test Loss: {best_test_loss:.4f}')\n",
    "\n",
    "    return best_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ce03f18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-10 13:15:21,999]\u001b[0m A new study created in memory with name: no-name-b1380a0e-6110-4755-b507-01a04c0e3a81\u001b[0m\n",
      "\u001b[32m[I 2023-05-10 13:30:06,050]\u001b[0m Trial 0 finished with value: inf and parameters: {'hidden_channels': 104, 'dropout': 0.2941561665361294, 'lr': 0.00023531793531438082, 'weight_decay': 0.0002447538053334466, 'batch_size': 16, 'activation_name': 'PReLU', 'optimizer_name': 'Adam'}. Best is trial 0 with value: inf.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 - Test Loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-10 13:44:46,190]\u001b[0m Trial 1 finished with value: inf and parameters: {'hidden_channels': 102, 'dropout': 0.499692217750518, 'lr': 0.002738639087260454, 'weight_decay': 0.0002017396180163049, 'batch_size': 16, 'activation_name': 'ReLU', 'optimizer_name': 'RMSprop'}. Best is trial 0 with value: inf.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 - Test Loss: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-10 13:52:13,679]\u001b[0m Trial 2 finished with value: inf and parameters: {'hidden_channels': 32, 'dropout': 0.29805881320309663, 'lr': 0.0006999518059467641, 'weight_decay': 5.384772942562328e-05, 'batch_size': 32, 'activation_name': 'PReLU', 'optimizer_name': 'Adagrad'}. Best is trial 0 with value: inf.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 - Test Loss: inf\n",
      "Best trial: 0, Test Loss: inf\n",
      "Best hyperparameters: {'hidden_channels': 104, 'dropout': 0.2941561665361294, 'lr': 0.00023531793531438082, 'weight_decay': 0.0002447538053334466, 'batch_size': 16, 'activation_name': 'PReLU', 'optimizer_name': 'Adam'}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'best_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m best_params \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 최적의 모델을 받아옵니다.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mbest_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_attrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 최적의 모델의 가중치를 저장합니다.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m best_model_weights \u001b[38;5;241m=\u001b[39m best_trial\u001b[38;5;241m.\u001b[39muser_attrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_weights\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'best_model'"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=3)\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best trial: {best_trial.number}, Test Loss: {best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {best_trial.params}\")\n",
    "\n",
    "# 최적의 파라미터를 세팅\n",
    "best_params = best_trial.params\n",
    "\n",
    "# 최적의 모델을 받아옵니다.\n",
    "best_model = best_trial.user_attrs['best_model']\n",
    "\n",
    "# 최적의 모델의 가중치를 저장합니다.\n",
    "best_model_weights = best_trial.user_attrs['best_model_weights']\n",
    "torch.save(best_model_weights, 'best_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a77dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fcfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "loaded_weights = torch.load('best_model_weights.pth')\n",
    "\n",
    "# best_model의 상태 사전 출력\n",
    "print(\"final_model's state_dict:\")\n",
    "for param_tensor in best_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", best_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"\\nLoaded weights:\")\n",
    "for param_tensor in loaded_weights:\n",
    "    print(param_tensor, \"\\t\", loaded_weights[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e97026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e3eace",
   "metadata": {},
   "source": [
    "## Train Model with Best Parameter\n",
    "\n",
    "1. 최적의 하이퍼파라미터를 찾습니다 (예: 그리드 서치, 랜덤 서치, 베이지안 최적화 등).\n",
    "2. 찾은 최적의 하이퍼파라미터를 사용하여 모델을 학습시킵니다.\n",
    "3. 학습 과정에서 가장 좋은 성능을 보여준 모델의 가중치를 저장합니다.\n",
    "4. 저장된 가중치를 불러와 본 적 없는 데이터셋에 대한 예측을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58dd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_params = {key: best_params[key] for key in ['hidden_channels', 'dropout', 'activation_name']}\n",
    "model_params['out_channels'] = 1\n",
    "model_params['in_channels'] = 30\n",
    "\n",
    "best_model = GraphSAGERegressor(**model_params).to(device)\n",
    "best_model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "\n",
    "optimizer_name = best_params['optimizer_name']\n",
    "optimizer_class = getattr(torch.optim, optimizer_name)\n",
    "lr = best_params['lr']\n",
    "weight_decay = best_params['weight_decay']\n",
    "optimizer = optimizer_class(best_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "batch_size = best_params['batch_size']\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[5, 3], batch_size=batch_size, shuffle=True, num_nodes=data.num_nodes)\n",
    "test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[5, 3], batch_size=batch_size, shuffle=False, num_nodes=data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "loaded_weights = torch.load('best_model_weights.pth')\n",
    "\n",
    "# best_model의 상태 사전 출력\n",
    "print(\"best_model's state_dict:\")\n",
    "for param_tensor in best_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", best_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"\\nLoaded weights:\")\n",
    "for param_tensor in loaded_weights:\n",
    "    print(param_tensor, \"\\t\", loaded_weights[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e535ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "num_epochs = 100  # 원하는 에포크 수를 설정하세요.\n",
    "patience = 10  # Early stopping patience 설정\n",
    "min_delta = 0.001  # Early stopping을 위한 최소 개선량 설정\n",
    "best_test_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Learning rate scheduler 설정\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "best_model.train()\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    train_loss = train(train_loader, optimizer)\n",
    "    test_loss = test(test_loader)\n",
    "    \n",
    "    # Learning rate scheduler 업데이트\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Early stopping 조건 검사\n",
    "    if test_loss < best_test_loss - min_delta:\n",
    "        best_test_loss = test_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f160c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'final_trained_model_weights.pth')\n",
    "\n",
    "with open('final_trained_model_params.json', 'w') as f:\n",
    "    json.dump(model_params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7f9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch_geometric.data import NeighborSampler\n",
    "# from torch.optim import AdamW\n",
    "\n",
    "# # Create the model with the best hyperparameters\n",
    "# model_params = {key: best_params[key] for key in ['hidden_channels', 'dropout', 'activation_name']}\n",
    "# model_params['out_channels'] = 1\n",
    "# best_model = GraphSAGERegressor(in_channels, **model_params).to(device)\n",
    "# best_model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "\n",
    "# # Create the optimizer with the best hyperparameters\n",
    "# optimizer_params = {key: best_params[key] for key in ['lr', 'weight_decay']}\n",
    "# optimizer = AdamW(best_model.parameters(), **optimizer_params)\n",
    "\n",
    "# # Create the data loaders with the best batch size\n",
    "# batch_size = best_params['batch_size']\n",
    "# train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[5, 3], batch_size=batch_size, shuffle=True)\n",
    "# test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[5, 3], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365ca61c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_val_loss = float('inf')\n",
    "# best_model = None\n",
    "# best_epoch = 0\n",
    "# patience = 5\n",
    "# counter = 0\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "# num_epochs = 100\n",
    "\n",
    "# for epoch in tqdm(range(1, num_epochs+1)):\n",
    "#     train_loss = train(train_loader, optimizer)\n",
    "#     print(f'Epoch {epoch:02d}, Train Loss: {train_loss:.4f}')\n",
    "#     scheduler.step()\n",
    "    \n",
    "#     test_loss = test(test_loader)\n",
    "#     print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "#     # Early Stopping\n",
    "#     if test_loss < best_val_loss:\n",
    "#         best_val_loss = test_loss\n",
    "#         global best_model\n",
    "#         global best_loss\n",
    "#         best_model = model\n",
    "#         torch.save(model.state_dict(), 'best_model_weights2.pth')  # 가중치 저장 위치 변경\n",
    "#         best_epoch = epoch\n",
    "# #         best_model_weights = copy.deepcopy(model.state_dict())\n",
    "# #         best_model = model.state_dict()\n",
    "# #         torch.save(best_model_weights, 'best_model_weights2.pth')  # best model 가중치 저장\n",
    "#         counter = 0            \n",
    "            \n",
    "#     else:\n",
    "#         counter += 1\n",
    "#         if counter >= patience:\n",
    "#             print(f'Early stopping: validation loss did not improve for {patience} epochs.')\n",
    "#             break\n",
    "            \n",
    "# print(f'Best validation loss of {best_val_loss:.4f} was achieved at epoch {best_epoch}.')\n",
    "# test_loss = test(test_loader)\n",
    "# print(f'Test loss of {test_loss:.4f} was achieved at epoch {best_epoch}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7823550",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_model2 = GraphSAGERegressor(**model_params).to(device)\n",
    "# best_model2.load_state_dict(torch.load('best_model_weights2.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac5677",
   "metadata": {},
   "source": [
    "# 여기서 다시 최종 모델 가중치 저장 (파라미터는 동일)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722161ed",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de282b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/prepro_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f1a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(test_df['Age'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_df['Location_encoded'] = le.fit_transform(test_df['Location'])\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=29)\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(test_df['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()\n",
    "\n",
    "title_embeddings = test_df['Book-Title'].apply(get_title_embedding_fasttext).tolist()\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)\n",
    "\n",
    "test_df['Publisher_encoded'] = le.fit_transform(test_df['Publisher'])\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=10)\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(test_df['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd100eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique_user_ids = test_df['User-ID'].unique().tolist()\n",
    "new_unique_book_ids = test_df['Book-ID'].unique().tolist()\n",
    "\n",
    "# 새로운 사용자 ID에 대한 인덱스를 할당합니다.\n",
    "last_used_index = 326696\n",
    "for new_user_id in new_unique_user_ids:\n",
    "    if new_user_id not in UserNodeID_dict:\n",
    "        last_used_index += 1\n",
    "        UserNodeID_dict[new_user_id] = last_used_index\n",
    "\n",
    "# 새로운 도서 ID에 대한 인덱스를 할당합니다.\n",
    "for new_book_id in new_unique_book_ids:\n",
    "    if new_book_id not in BookNodeID_dict:\n",
    "        last_used_index += 1\n",
    "        BookNodeID_dict[new_book_id] = last_used_index\n",
    "        \n",
    "# IDs 매핑 진행\n",
    "test_df['User-ID'] = test_df['User-ID'].map(UserNodeID_dict)\n",
    "test_df['Book-ID'] = test_df['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b137184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique_node_count = len(UserNodeID_dict) + len(BookNodeID_dict)\n",
    "new_feature_matrix = np.zeros((new_unique_node_count, feature_dim))\n",
    "\n",
    "user_ids = test_df['User-ID'].unique().tolist()  # 유니크 처리함\n",
    "book_ids = test_df['Book-ID'].unique().tolist()\n",
    "\n",
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "# 스케일링을 개별적으로 수행\n",
    "age_tensor_scaled = scaler.fit_transform(age_tensor.reshape(-1, 1))\n",
    "location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor_scaled, location_embeddings_scaled):\n",
    "    new_feature_matrix[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "title_embeddings_scaled = scaler.fit_transform(reduced_title_embeddings)\n",
    "publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, title_embeddings_scaled, publisher_embeddings_scaled):\n",
    "    new_feature_matrix[book_id - num_user_nodes] = np.concatenate([title, publisher], axis=0)\n",
    "\n",
    "new_node_feature_matrix = torch.tensor(new_feature_matrix, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3654e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_node_feature_matrix.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f5825",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_node_feature_matrix.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157004dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_params = {key: best_params[key] for key in ['hidden_channels', 'dropout', 'activation_name']}\n",
    "# model_params['out_channels'] = 1\n",
    "# model_params['in_channels'] = 1\n",
    "\n",
    "# # batch_size = best_params['batch_size']\n",
    "# best_model = GraphSAGERegressor(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 가중치 불러오기\n",
    "# loaded_weights = torch.load('best_model_weights.pth')\n",
    "\n",
    "# # best_model의 상태 사전 출력\n",
    "# print(\"best_model's state_dict:\")\n",
    "# for param_tensor in best_model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", best_model.state_dict()[param_tensor].size())\n",
    "\n",
    "# print(\"\\nLoaded weights:\")\n",
    "# for param_tensor in loaded_weights:\n",
    "#     print(param_tensor, \"\\t\", loaded_weights[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdf0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights from the training process\n",
    "# best_model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "# best_model = best_model.to(device)\n",
    "\n",
    "\n",
    "with open('final_trained_model_params.json', 'r') as f:\n",
    "    loaded_model_params = json.load(f)\n",
    "    \n",
    "final_model = GraphSAGERegressor(**loaded_model_params).to(device)\n",
    "\n",
    "final_model.load_state_dict(torch.load('final_trained_model_weights.pth'))\n",
    "\n",
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff4d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "loaded_weights = torch.load('final_trained_model_weights.pth')\n",
    "\n",
    "# best_model의 상태 사전 출력\n",
    "print(\"final_model's state_dict:\")\n",
    "for param_tensor in final_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", best_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"\\nLoaded weights:\")\n",
    "for param_tensor in loaded_weights:\n",
    "    print(param_tensor, \"\\t\", loaded_weights[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d60f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edge_index = torch.tensor(test_df[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()\n",
    "new_data = Data(x=new_node_feature_matrix, edge_index=new_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd354041",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_loader = DataLoader([new_data], batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5404215",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in new_data_loader:\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = final_model(batch.x, [batch.edge_index])\n",
    "        edge_index_row, edge_index_col = batch.edge_index\n",
    "        edge_predictions = out[edge_index_row] * out[edge_index_col]\n",
    "        predictions.append(edge_predictions.sum(dim=-1).cpu().numpy())\n",
    "\n",
    "# Combine the predictions\n",
    "predictions = np.concatenate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62170e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5daf29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20533e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5291931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb07802d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2dbbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
