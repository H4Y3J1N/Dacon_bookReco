{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95d5a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn import Embedding\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fcea759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8df8c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./data/prepro_train_data.csv\")\n",
    "test_df = pd.read_csv(\"./data/prepro_test_data.csv\")\n",
    "submit = pd.read_csv(\"./data/sample_submission.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d79aaaf",
   "metadata": {},
   "source": [
    "# Make Graph Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710b99b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(dataset['Age'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "### Location\n",
    "# LabelEncoder를 사용해 위치 정보를 정수로 변환\n",
    "le = LabelEncoder()\n",
    "dataset['Location_encoded'] = le.fit_transform(dataset['Location'])\n",
    "\n",
    "# 임베딩 레이어 초기화\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=79)\n",
    "\n",
    "# 위치 정보를 10차원 벡터로 변환\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "\n",
    "# 결과를 NumPy array로 변환\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e711d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "# FastText 모델 로드\n",
    "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")\n",
    "\n",
    "def get_title_embedding_fasttext(title):\n",
    "    if not isinstance(title, str):\n",
    "        title = \"\"\n",
    "    words = title.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    embeddings = [fasttext_model.get_word_vector(word) for word in words]\n",
    "    return np.mean(embeddings, axis=0)\n",
    "title_embeddings = dataset['Book-Title'].apply(get_title_embedding_fasttext).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d634b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# PCA 객체를 생성\n",
    "pca = PCA(n_components=50)\n",
    "\n",
    "# 평균 임베딩 벡터로 구성된 리스트를 NumPy 배열로 변환\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "\n",
    "# 차원 축소\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a75c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Publisher\n",
    "# LabelEncoder를 사용해 위치 정보를 정수로 변환\n",
    "le = LabelEncoder()\n",
    "dataset['Publisher_encoded'] = le.fit_transform(dataset['Publisher'])\n",
    "\n",
    "# 임베딩 레이어 초기화\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=30)\n",
    "\n",
    "# 위치 정보를 10차원 벡터로 변환\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "\n",
    "# 결과를 NumPy array로 변환\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d656c606",
   "metadata": {},
   "source": [
    "### User-ID & Book-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eb809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([dataset, test_df])\n",
    "\n",
    "unique_user_ids_num = combined_df['User-ID'].nunique()\n",
    "unique_book_ids_num = combined_df['Book-ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33592e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "# User-ID 열의 unique한 값들을 리스트로 만들기  > 83256\n",
    "unique_user_ids = combined_df['User-ID'].unique().tolist()\n",
    "# Book-ID 열의 unique한 값들을 리스트로 만들기 > 243441\n",
    "unique_book_ids = combined_df['Book-ID'].unique().tolist()\n",
    "\n",
    "# unique_user_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_user_ids_df = unique_user_ids_df.sort_values(by='User-ID', ascending=True)\n",
    "# # 인덱스를 새 column으로 추가\n",
    "sorted_unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_user_ids_df.rename(columns={'index': 'UserNodeID'}, inplace=True)\n",
    "\n",
    "# unique_book_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_book_ids_df = unique_book_ids_df.sort_values(by='Book-ID', ascending=True)\n",
    "# #인덱스를 새 column으로 추가\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_book_ids_df.rename(columns={'index': 'BookNodeID'}, inplace=True)\n",
    "\n",
    "\n",
    "# # book_ids 인덱스 시작 번호를 83256으로 설정\n",
    "sorted_unique_book_ids_df['BookNodeID'] += (unique_user_ids_num)\n",
    "# UserNodeID  0 ~ 83255 ....인데 test 데이터셋의 uniuqe 숫자까지 고려하면 92102.\n",
    "# BookNodeID   83256(/////92101) ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907111bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ID_dict(df):\n",
    "    ID_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        ID_dict[row[1]] = row[0]\n",
    "    return ID_dict\n",
    "\n",
    "UserNodeID_dict = make_ID_dict(sorted_unique_user_ids_df)\n",
    "BookNodeID_dict = make_ID_dict(sorted_unique_book_ids_df)\n",
    "\n",
    "####### 매핑 진행\n",
    "dataset['User-ID'] = dataset['User-ID'].map(UserNodeID_dict)\n",
    "dataset['Book-ID'] = dataset['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfb633ef",
   "metadata": {},
   "source": [
    "## node feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ac119e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_user_nodes = len(dataset['User-ID'].unique().tolist())\n",
    "# num_book_nodes = len(dataset['Book-ID'].unique().tolist())\n",
    "\n",
    "user_ids = dataset['User-ID'].unique().tolist()  # 유니크 처리함\n",
    "book_ids = dataset['Book-ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6efb7ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_users = max(user_ids) + 1\n",
    "# num_books = max(book_ids) - num_user_nodes + 1\n",
    "\n",
    "feature_dim = 80  # age (1) + location (29)  # title (20) + publisher (10)\n",
    "\n",
    "feature_matrix = np.zeros((unique_user_ids_num+unique_book_ids_num, feature_dim))\n",
    "# book_features = np.zeros((unique_book_ids_num, feature_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "891b5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 스케일링을 개별적으로 수행\n",
    "age_tensor_scaled = scaler.fit_transform(age_tensor.reshape(-1, 1))\n",
    "location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor_scaled, location_embeddings_scaled):\n",
    "    feature_matrix[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "title_embeddings_scaled = scaler.fit_transform(reduced_title_embeddings)\n",
    "publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, title_embeddings_scaled, publisher_embeddings_scaled):\n",
    "    feature_matrix[book_id] = np.concatenate([title, publisher], axis=0)\n",
    "    \n",
    "# 사용자 노드 행렬과 도서 노드 행렬을 세로로 연결하여 최종 node_features 행렬 생성:    \n",
    "# node_features = np.vstack((user_features, book_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47d3527f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "node_feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)\n",
    "edge_attr = torch.tensor(dataset['Book-Rating'].values, dtype=torch.float).unsqueeze(-1)\n",
    "edge_index = torch.tensor(dataset[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "data = Data(x=node_feature_matrix,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210f26aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[326697, 80], edge_index=[2, 871393], edge_attr=[871393, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2530a023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_ids = np.unique(dataset['User-ID'].values)\n",
    "train_book_ids = np.unique(dataset['Book-ID'].values)\n",
    "train_node_ids = np.concatenate((train_user_ids, train_book_ids))\n",
    "train_idx = torch.tensor(train_node_ids, dtype=torch.long)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f467d9e2",
   "metadata": {},
   "source": [
    "# GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8c5e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.nn import Embedding\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fed04cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GraphSAGE(data.num_node_features, 128, 64).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    data = data.to(device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(200):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data.x, data.edge_index)  # Obtain node embeddings\n",
    "\n",
    "        # Negative sampling\n",
    "        edge_index_pos = data.edge_index\n",
    "        edge_index_neg = negative_sampling(edge_index_pos, num_nodes=data.num_nodes,\n",
    "                                           num_neg_samples=edge_index_pos.size(1))\n",
    "\n",
    "        pos_loss = (1 - torch.sigmoid((z[edge_index_pos[0]] * z[edge_index_pos[1]]).sum(dim=-1))).mean()\n",
    "        neg_loss = torch.sigmoid((z[edge_index_neg[0]] * z[edge_index_neg[1]]).sum(dim=-1)).mean()\n",
    "        loss = -torch.log(pos_loss) - torch.log(1 - neg_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a37f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edea4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = data.x.to(torch.device('cuda'))\n",
    "    edge_index = data.edge_index.to(torch.device('cuda'))\n",
    "    embeddings = model(x, edge_index).cpu().detach().numpy()\n",
    "\n",
    "# # 임베딩 출력\n",
    "# for i, embedding in enumerate(embeddings):\n",
    "#     node_label = data.classes[data.y[i].item()]\n",
    "#     print(f\"Node {i}: Label={node_label}, Embedding={embedding}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc86b241",
   "metadata": {},
   "source": [
    "# LGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a216a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a4586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[[\"Age\",\"Location_encoded\",\"Year-Of-Publication\",\"Publisher_encoded\"]]\n",
    "df_np = df.to_numpy()\n",
    "book_title_embeddings = np.stack(dataset[\"Book-Title_encoded\"].values)\n",
    "df_emb = np.concatenate((df_np, book_title_embeddings), axis=1) # features_emb는 edge_index와 완전히 같은 정렬 순서를 가짐\n",
    "edge_index = edge_index.cpu().numpy()\n",
    "# 이제 여기에 embeddings에서 적절한 값들을 concat하면 된다.\n",
    "\n",
    "# edge_index[0] 리스트를 순회하며, 해당 값으로 embeddings를 인덱싱\n",
    "user_embeddings_selected = embeddings[edge_index[0]]\n",
    "\n",
    "# 인덱싱된 값을 그대로 df_emb의 오른편에 concat\n",
    "df_emb = np.concatenate((df_emb, user_embeddings_selected), axis=1)\n",
    "\n",
    "# edge_index[1] 리스트를 순회하며, 해당 값으로 embeddings를 인덱싱\n",
    "book_embeddings_selected = embeddings[edge_index[1]]\n",
    "\n",
    "# 인덱싱된 값을 그대로 df_emb의 오른편에 concat\n",
    "df_emb = np.concatenate((df_emb, book_embeddings_selected), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a372ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features and targets\n",
    "X = df_emb\n",
    "y = data.edge_attr.cpu().numpy()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf8a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 5개 파라미터 세트 출력하는 버전 \n",
    "\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 0.5, 1.0),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction', 0.7, 1.0),\n",
    "    'bagging_freq': hp.quniform('bagging_freq', 1, 7, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 10, 1),\n",
    "    'force_col_wise': hp.choice('force_col_wise', [True]),\n",
    "}\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    params = {\n",
    "        'device': 'gpu',\n",
    "        'num_leaves': int(params['num_leaves']),\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'feature_fraction': params['feature_fraction'],\n",
    "        'bagging_fraction': params['bagging_fraction'],\n",
    "        'bagging_freq': int(params['bagging_freq']),\n",
    "        'max_depth': int(params['max_depth']),\n",
    "        'force_col_wise': params['force_col_wise'],\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # This is a 5-fold cross-validation\n",
    "    cv_results = lgb.cv(params, train_data, num_boost_round=500, nfold=5, \n",
    "                        early_stopping_rounds=50, metrics='rmse', seed=42)\n",
    "    # Hyperopt will try to minimize loss (it always minimizes the objective)\n",
    "    loss = min(cv_results['rmse-mean'])\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "# Run the algorithm\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\")\n",
    "print(best)\n",
    "\n",
    "# Print top 5 hyperparameter sets\n",
    "sorted_trials = sorted(trials.results, key=lambda x: x['loss'])\n",
    "print(\"Top 5 hyperparameter sets:\")\n",
    "for t in sorted_trials[:5]:\n",
    "    print(f\"Loss: {t['loss']}, Params: {t['params']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83011dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # sorted_trials 변수 저장\n",
    "# with open(\"sorted_trials.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(sorted_trials, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47a7c285",
   "metadata": {},
   "source": [
    "# train LGBMs for Amsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b2f602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9e162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM model\n",
    "\n",
    "\n",
    "models = {}\n",
    "# 각각의 하이퍼파라미터 세트에 대해 모델을 학습하고 저장\n",
    "for i, params in enumerate(hyperparameters):\n",
    "    gbm = lgb.train(params, train_data, num_boost_round=500, valid_sets=test_data,\n",
    "                early_stopping_rounds=10, verbose_eval=False)\n",
    "    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    # Calculate and print RMSE\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    models[i] = gbm\n",
    "\n",
    "# 각 모델을 파일로 저장\n",
    "for i, model in models.items():\n",
    "    model.save_model(f'model_{i}.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f75cf849",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b5537",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(test_df['Age'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_df['Location_encoded'] = le.fit_transform(test_df['Location'])\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=79)\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(test_df['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "title_embeddings = test_df['Book-Title'].apply(get_title_embedding_fasttext).tolist()\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)\n",
    "\n",
    "test_df['Publisher_encoded'] = le.fit_transform(test_df['Publisher'])\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=30)\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(test_df['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()\n",
    "\n",
    "# new_unique_user_ids = test_df['User-ID'].unique().tolist()\n",
    "# new_unique_book_ids = test_df['Book-ID'].unique().tolist()\n",
    "\n",
    "# # 새로운 사용자 ID에 대한 인덱스를 할당합니다.\n",
    "# last_used_index = 326696\n",
    "# for new_user_id in new_unique_user_ids:\n",
    "#     if new_user_id not in UserNodeID_dict:\n",
    "#         last_used_index += 1\n",
    "#         UserNodeID_dict[new_user_id] = last_used_index\n",
    "        \n",
    "# book_index_start = last_used_index\n",
    "# # 새로운 도서 ID에 대한 인덱스를 할당합니다.\n",
    "# for new_book_id in new_unique_book_ids:\n",
    "#     if new_book_id not in BookNodeID_dict:\n",
    "#         last_used_index += 1\n",
    "#         BookNodeID_dict[new_book_id] = last_used_index\n",
    "        \n",
    "# IDs 매핑 진행\n",
    "test_df['User-ID'] = test_df['User-ID'].map(UserNodeID_dict)\n",
    "test_df['Book-ID'] = test_df['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_ids = test_df['User-ID'].unique().tolist()  # 유니크 처리함\n",
    "# book_ids = test_df['Book-ID'].unique().tolist()\n",
    "\n",
    "# new_unique_node_count = len(user_ids) + len(book_ids)\n",
    "# new_feature_matrix = np.zeros((new_unique_node_count, feature_dim))\n",
    "\n",
    "\n",
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "# 스케일링을 개별적으로 수행\n",
    "age_tensor_scaled = scaler.fit_transform(age_tensor.reshape(-1, 1))\n",
    "location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor_scaled, location_embeddings_scaled):\n",
    "    feature_matrix[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "title_embeddings_scaled = scaler.fit_transform(reduced_title_embeddings)\n",
    "publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, title_embeddings_scaled, publisher_embeddings_scaled):\n",
    "    feature_matrix[book_id] = np.concatenate([title, publisher], axis=0)\n",
    "\n",
    "new_node_feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96985c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_node_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd111343",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edge_index = torch.tensor(test_df[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous().to(device)\n",
    "new_data = Data(x=new_node_feature_matrix, edge_index=new_edge_index).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 추출\n",
    "model = train(data)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x = new_data.x.to(torch.device('cuda'))\n",
    "    edge_index = new_edge_index.to(torch.device('cuda'))\n",
    "    test_embeddings = model(x, new_edge_index).cpu().detach().numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "445a53ea",
   "metadata": {},
   "source": [
    "### 최종 예측을 위한 feature 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c02cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Book-Title_encoded'] = test_df['Book-Title'].apply(get_title_embedding_fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df[[\"Age\",\"Location_encoded\",\"Year-Of-Publication\",\"Publisher_encoded\"]]\n",
    "df_np = df.to_numpy()\n",
    "book_title_embeddings = np.stack(test_df[\"Book-Title_encoded\"].values)\n",
    "df_emb = np.concatenate((df_np, book_title_embeddings), axis=1) \n",
    "new_edge_index = new_edge_index.cpu().numpy()\n",
    "# features_emb는 edge_index와 완전히 같은 정렬 순서를 가진다.\n",
    "\n",
    "# edge_index[0] 리스트를 순회하며, 해당 값으로 embeddings를 인덱싱\n",
    "user_embeddings_selected = test_embeddings[new_edge_index[0]]\n",
    "\n",
    "# 인덱싱된 값을 그대로 df_emb의 오른편에 concat\n",
    "df_emb = np.concatenate((df_emb, user_embeddings_selected), axis=1)\n",
    "\n",
    "# edge_index[1] 리스트를 순회하며, 해당 값으로 embeddings를 인덱싱\n",
    "book_embeddings_selected = test_embeddings[new_edge_index[1]]\n",
    "\n",
    "# 인덱싱된 값을 그대로 df_emb의 오른편에 concat\n",
    "df_emb = np.concatenate((df_emb, book_embeddings_selected), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5bce71",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = []\n",
    "\n",
    "for model in models.values():\n",
    "    pred = model.predict(df_emb)\n",
    "    new_predictions.append(pred)\n",
    "    \n",
    "# 앙상블: 예측값들의 평균을 구함\n",
    "final_new_predictions = np.mean(new_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df94f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['Book-Rating'] = final_new_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389d224e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adfa40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671b9a78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
