{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, NeighborSampler, Batch, DataLoader\n",
    "from torch_geometric.nn import SAGEConv, GAE, TopKPooling\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import Embedding\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader as api\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./prepro_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Book-Rating' 컬럼 값의 빈도수 계산\n",
    "rating_counts = dataset['Book-Rating'].value_counts()\n",
    "\n",
    "# 결과 출력\n",
    "print(rating_counts)\n",
    "\n",
    "# 심각한 class imbalance 문제"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Graph\n",
    "## Category Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(dataset['Age'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"age_tensor.shape >>> \",age_tensor.shape)\n",
    "print(age_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Location\n",
    "\n",
    "le = LabelEncoder()\n",
    "dataset['Location_encoded'] = le.fit_transform(dataset['Location'])\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=29)\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"location_embeddings.shape >>> \",location_embeddings.shape)\n",
    "print(location_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book-Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# # 사전 훈련된 FastText 모델 다운로드\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "# !gunzip cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_embedding_fasttext(title):\n",
    "    words = title.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    embeddings = [fasttext_model.get_word_vector(word) for word in words]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = dataset['Book-Title'].apply(get_title_embedding_fasttext).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_titles_count = 0\n",
    "for title in dataset['Book-Title']:\n",
    "    words = title.split()\n",
    "    if not words:\n",
    "        empty_titles_count += 1\n",
    "\n",
    "print(f\"빈 문자열이거나 토큰화된 단어가 없는 책 제목의 개수: {empty_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, emb in enumerate(title_embeddings):\n",
    "    if not isinstance(emb, np.ndarray) or emb.shape != (300,):\n",
    "        print(f\"Index: {idx}, Title: {dataset['Book-Title'][idx]}, Embedding: {emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reduced_title_embeddings.shape >>> \",reduced_title_embeddings.shape)\n",
    "print(reduced_title_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Publisher\n",
    "\n",
    "le = LabelEncoder()\n",
    "dataset['Publisher_encoded'] = le.fit_transform(dataset['Publisher'])\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=10)\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Publisher_embeddings.shape >>> \",Publisher_embeddings.shape)\n",
    "print(Publisher_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-ID & Book-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "# User-ID 열의 unique한 값들을 리스트로 만들기  > 83256\n",
    "unique_user_ids = dataset['User-ID'].unique().tolist()\n",
    "# Book-ID 열의 unique한 값들을 리스트로 만들기 > 243441\n",
    "unique_book_ids = dataset['Book-ID'].unique().tolist()\n",
    "\n",
    "# unique_user_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_user_ids_df = unique_user_ids_df.sort_values(by='User-ID', ascending=True)\n",
    "# # 인덱스를 새 column으로 추가\n",
    "sorted_unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_user_ids_df.rename(columns={'index': 'UserNodeID'}, inplace=True)\n",
    "\n",
    "# unique_book_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_book_ids_df = unique_book_ids_df.sort_values(by='Book-ID', ascending=True)\n",
    "# #인덱스를 새 column으로 추가\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_book_ids_df.rename(columns={'index': 'BookNodeID'}, inplace=True)\n",
    "\n",
    "# 인덱스 시작 번호를 83256으로 설정\n",
    "sorted_unique_book_ids_df['BookNodeID'] += 83256\n",
    "\n",
    "# UserNodeID  0 ~ 83255\n",
    "# BookNodeID   83256 ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ID_dict(df):\n",
    "    ID_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        ID_dict[row[1]] = row[0]\n",
    "    return ID_dict\n",
    "\n",
    "UserNodeID_dict = make_ID_dict(sorted_unique_user_ids_df)\n",
    "BookNodeID_dict = make_ID_dict(sorted_unique_book_ids_df)\n",
    "\n",
    "####### 매핑 진행\n",
    "dataset['User-ID'] = dataset['User-ID'].map(UserNodeID_dict)\n",
    "dataset['Book-ID'] = dataset['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_nodes = len(dataset['User-ID'].unique().tolist())\n",
    "num_book_nodes = len(dataset['Book-ID'].unique().tolist())\n",
    "\n",
    "user_ids = dataset['User-ID'].unique().tolist()  # 유니크 처리함\n",
    "book_ids = dataset['Book-ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = max(user_ids) + 1\n",
    "num_books = max(book_ids) - num_user_nodes + 1\n",
    "\n",
    "feature_dim = 30  # age (1) + location (29)  # title (20) + publisher (10)\n",
    "\n",
    "user_features = np.zeros((num_user_nodes, feature_dim))\n",
    "book_features = np.zeros((num_book_nodes, feature_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "# 스케일링\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 스케일링을 개별적으로 수행\n",
    "age_tensor_scaled = scaler.fit_transform(age_tensor.reshape(-1, 1))\n",
    "location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor_scaled, location_embeddings_scaled):\n",
    "    user_features[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "title_embeddings_scaled = scaler.fit_transform(reduced_title_embeddings)\n",
    "publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, title_embeddings_scaled, publisher_embeddings_scaled):\n",
    "    book_features[book_id - num_user_nodes] = np.concatenate([title, publisher], axis=0)\n",
    "    \n",
    "# 사용자 노드 행렬과 도서 노드 행렬을 세로로 연결하여 최종 node_features 행렬 생성:    \n",
    "node_features = np.vstack((user_features, book_features))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Split - Node base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 노드의 개수를 계산\n",
    "num_nodes = len(node_features)\n",
    "\n",
    "# 데이터 분할을 위한 나눔\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# train 셋에만 오버샘플링 적용\n",
    "from sklearn.utils import resample\n",
    "n_samples = 60000\n",
    "df_list = [train_data[train_data['Book-Rating'] == i] for i in range(11)]\n",
    "resampled_df_list = [resample(df, replace=True, n_samples=n_samples, random_state=42) for df in df_list]\n",
    "resampled_train_data = pd.concat(resampled_df_list)\n",
    "\n",
    "# 각각의 데이터프레임에서 유니크한 값을 찾아냄\n",
    "train_user_ids = np.unique(resampled_train_data['User-ID'].values)\n",
    "train_book_ids = np.unique(resampled_train_data['Book-ID'].values)\n",
    "test_user_ids = np.unique(test_data['User-ID'].values)\n",
    "test_book_ids = np.unique(test_data['Book-ID'].values)\n",
    "\n",
    "#유니크한 값들을 이어붙임\n",
    "train_node_ids = np.concatenate((train_user_ids, train_book_ids))\n",
    "test_node_ids = np.concatenate((test_user_ids, test_book_ids))\n",
    "\n",
    "# 텐서로 만듦\n",
    "train_idx = torch.tensor(train_node_ids, dtype=torch.long)\n",
    "test_idx = torch.tensor(test_node_ids, dtype=torch.long)\n",
    "\n",
    "# 마스크 생성. 처음에는 모든 값이 False로 초기화되어 있음\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "#마스크 적용. 각 마스크에 대해 앞서 추출한 노드 ID에 해당하는 인덱스를 True로 설정.\n",
    "# 이렇게 하면 각 마스크는 해당 데이터셋에 속하는 노드를 나타내게 된다.\n",
    "train_mask[train_node_ids] = True\n",
    "test_mask[test_node_ids] = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = torch.tensor(dataset['Book-Rating'].values, dtype=torch.float).unsqueeze(-1)\n",
    "y = edge_attr.clone()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ratings = pd.concat([resampled_train_data, test_data]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(combined_ratings[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()\n",
    "edge_index # torch.Size([2, 871393])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할된 후에 다시 클래스 분포 확인해 보기. \n",
    "\n",
    "test_data_rating_counts = test_data['Book-Rating'].value_counts()\n",
    "print(test_data_rating_counts)\n",
    "\n",
    "resampled_train_data_rating_counts = resampled_train_data['Book-Rating'].value_counts()\n",
    "print(resampled_train_data_rating_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 데이터 생성\n",
    "data = Data(x=node_feature_matrix,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=y,\n",
    "            train_mask=train_mask,\n",
    "            test_mask=test_mask)\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플러에 적용\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[5, 3], batch_size=32, shuffle=True, num_nodes=data.num_nodes)\n",
    "test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[5, 3], batch_size=32, shuffle=False, num_nodes=data.num_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class WeightedSAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(WeightedSAGEConv, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        row, col = edge_index\n",
    "        if edge_attr is not None:\n",
    "            edge_attr = edge_attr[col]  # 인접 노드에 대한 edge_attr만 선택\n",
    "        edge_index, edge_attr = add_self_loops(edge_index, edge_attr, num_nodes=x.size(0))\n",
    "        x = self.lin(x)\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        if edge_attr is not None:\n",
    "            return x_j * edge_attr.view(-1, 1)\n",
    "        else:\n",
    "            return x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGERegressor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout, activation_name, num_layers=2):\n",
    "        super(GraphSAGERegressor, self).__init__()\n",
    "\n",
    "        # Select the activation function\n",
    "        activations = torch.nn.ModuleDict([\n",
    "            ['ReLU', torch.nn.ReLU()],\n",
    "            ['ReLU6', torch.nn.ReLU6()],\n",
    "            ['LeakyReLU', torch.nn.LeakyReLU()],\n",
    "            ['PReLU', torch.nn.PReLU()],\n",
    "            ['ELU', torch.nn.ELU()],\n",
    "            ['SiLU', torch.nn.SiLU()]\n",
    "        ])\n",
    "        self.activation = activations[activation_name]\n",
    "        if self.activation is None:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_name}\")\n",
    "\n",
    "        self.user_convs = torch.nn.ModuleList()\n",
    "        self.book_convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Add the first layer\n",
    "        self.user_convs.append(WeightedSAGEConv(in_channels, hidden_channels))\n",
    "        self.book_convs.append(WeightedSAGEConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Add intermediate layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.user_convs.append(WeightedSAGEConv(hidden_channels, hidden_channels))\n",
    "            self.book_convs.append(WeightedSAGEConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Add the last layer\n",
    "        self.user_convs.append(WeightedSAGEConv(hidden_channels, out_channels))\n",
    "        self.book_convs.append(WeightedSAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index_list, edge_attr_list=None):\n",
    "        if edge_attr_list is None:\n",
    "            edge_attr_list = [None] * len(edge_index_list)\n",
    "            \n",
    "        user_x = x\n",
    "        book_x = x\n",
    "\n",
    "        for i in range(len(self.user_convs) - 1):\n",
    "            user_x = self.user_convs[i](user_x, edge_index_list[i], edge_attr_list[i])\n",
    "            user_x = self.batch_norms[i](user_x)\n",
    "            user_x = self.activation(user_x)\n",
    "            user_x = self.dropout(user_x)\n",
    "\n",
    "            book_x = self.book_convs[i](book_x, edge_index_list[i], edge_attr_list[i])\n",
    "            book_x = self.batch_norms[i](book_x)\n",
    "            book_x = self.activation(book_x)\n",
    "            book_x = self.dropout(book_x)\n",
    "\n",
    "        user_emb = self.user_convs[-1](user_x, edge_index_list[-1], edge_attr_list[-1])\n",
    "        user_emb = torch.sigmoid(user_emb) * 10\n",
    "\n",
    "        book_emb = self.book_convs[-1](book_x, edge_index_list[-1], edge_attr_list[-1])\n",
    "        book_emb = torch.sigmoid(book_emb) * 10\n",
    "\n",
    "        return user_emb, book_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_channels = 30\n",
    "hidden_channels = 32\n",
    "out_channels = 1\n",
    "activation_name = 'ReLU'\n",
    "dropout =  0.1\n",
    "\n",
    "model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout, activation_name, num_layers=3)\n",
    "model = model.to(device)  # 모델만 옮기고, 데이터는 옮기지 않는다. 그래야 메모리 효율성 업!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        adjs = [adj.to(device) for adj in adjs]  # Move adjs to device\n",
    "        edge_index_list = [adj.edge_index for adj in adjs]\n",
    "        edge_attr_list = [data.edge_attr.to(device) for _ in adjs]  # Reuse edge_attr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "        user_emb, book_emb = out[:batch_size]  # Assume the model outputs user embeddings and book embeddings\n",
    "        predictions = (user_emb * book_emb).sum(dim=-1)  # Compute predicted ratings\n",
    "        loss = criterion(predictions, data.y[n_id[:batch_size]].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in test_loader:\n",
    "        adjs = [adj.to(device) for adj in adjs]  # Move adjs to device\n",
    "        edge_index_list = [adj.edge_index for adj in adjs]\n",
    "        edge_attr_list = [data.edge_attr.to(device) for _ in adjs]\n",
    "        \n",
    "        out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "        user_emb, book_emb = out[:batch_size]  # Assume the model outputs user embeddings and book embeddings\n",
    "        predictions = (user_emb * book_emb).sum(dim=-1)  # Compute predicted ratings\n",
    "        loss = criterion(predictions, data.y[n_id[:batch_size]].to(device))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beysian Hyper Parameter Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # 하이퍼파라미터 추천값 설정\n",
    "    in_channels = node_features.shape[1]\n",
    "    hidden_channels = trial.suggest_int('hidden_channels', 32, 128)\n",
    "    out_channels = 1\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    activation_name = trial.suggest_categorical('activation_name', ['ReLU', 'LeakyReLU', 'PReLU', 'ELU', 'SiLU'])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer_name', ['AdamW', 'Adam', 'RMSprop', 'Adagrad'])\n",
    "\n",
    "    #hidden_channels, dropout 등 숫자 값을 설정할 때, 가능한 경우 integer가 아닌 float으로 설정하는 것이 좋습니다.\n",
    "    # 이렇게 함으로써 float값 범위 내에서 모든 가능한 값을 테스트해볼 수 있게 됩니다.\n",
    "    \n",
    "    # 모델 및 최적화 생성\n",
    "    model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout, activation_name).to(device)\n",
    "\n",
    "    optimizer_class = getattr(torch.optim, optimizer_name)\n",
    "    optimizer_instance = optimizer_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    #최적화 함수를 바꿀 때마다 optimizer_instance의 파라미터를 수정해줘야 하는데, 이 과정에서 누락될 가능성이 있습니다.\n",
    "    # 이를 방지하기 위해 optimizer_instance를 새로 생성해주는 것이 좋습니다.\n",
    "    \n",
    "    # DataLoader 수정\n",
    "    train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[5,3], batch_size=batch_size, shuffle=True, num_nodes=data.num_nodes)\n",
    "    test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[5, 3], batch_size=batch_size, shuffle=False, num_nodes=data.num_nodes)\n",
    "\n",
    "    # 학습 및 평가 루프\n",
    "    best_test_loss = float('inf')\n",
    "    for epoch in range(1, 11):\n",
    "        train_loss = train(train_loader, optimizer_instance)\n",
    "        test_loss = test(test_loader)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_model_weights = deepcopy(model.state_dict())\n",
    "            trial.set_user_attr('best_model_weights', best_model_weights)\n",
    "            trial.set_user_attr('best_model', model)\n",
    "    tqdm.write(f'Trial {trial.number} - Test Loss: {best_test_loss:.4f}')\n",
    "\n",
    "    return best_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=3)\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best trial: {best_trial.number}, Test Loss: {best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {best_trial.params}\")\n",
    "\n",
    "# 최적의 파라미터를 세팅\n",
    "best_params = best_trial.params\n",
    "\n",
    "# 최적의 모델을 받아옵니다.\n",
    "best_model = best_trial.user_attrs['best_model']\n",
    "\n",
    "# 최적의 모델의 가중치를 저장합니다.\n",
    "best_model_weights = best_trial.user_attrs['best_model_weights']\n",
    "torch.save(best_model_weights, 'best_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "loaded_weights = torch.load('best_model_weights.pth')\n",
    "\n",
    "# best_model의 상태 사전 출력\n",
    "print(\"final_model's state_dict:\")\n",
    "for param_tensor in best_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", best_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"\\nLoaded weights:\")\n",
    "for param_tensor in loaded_weights:\n",
    "    print(param_tensor, \"\\t\", loaded_weights[param_tensor].size())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Best Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {key: best_params[key] for key in ['hidden_channels', 'dropout', 'activation_name']}\n",
    "model_params['out_channels'] = 1\n",
    "model_params['in_channels'] = 30\n",
    "\n",
    "best_model = GraphSAGERegressor(**model_params).to(device)\n",
    "best_model.load_state_dict(torch.load('best_model_weights.pth'))\n",
    "\n",
    "optimizer_name = best_params['optimizer_name']\n",
    "optimizer_class = getattr(torch.optim, optimizer_name)\n",
    "lr = best_params['lr']\n",
    "weight_decay = best_params['weight_decay']\n",
    "optimizer = optimizer_class(best_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "batch_size = best_params['batch_size']\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx, sizes=[5, 3], batch_size=batch_size, shuffle=True, num_nodes=data.num_nodes)\n",
    "test_loader = NeighborSampler(data.edge_index, node_idx=test_idx, sizes=[5, 3], batch_size=batch_size, shuffle=False, num_nodes=data.num_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "loaded_weights = torch.load('best_model_weights.pth')\n",
    "\n",
    "# best_model의 상태 사전 출력\n",
    "print(\"best_model's state_dict:\")\n",
    "for param_tensor in best_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", best_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"\\nLoaded weights:\")\n",
    "for param_tensor in loaded_weights:\n",
    "    print(param_tensor, \"\\t\", loaded_weights[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "num_epochs = 100  # 원하는 에포크 수를 설정하세요.\n",
    "patience = 10  # Early stopping patience 설정\n",
    "min_delta = 0.001  # Early stopping을 위한 최소 개선량 설정\n",
    "best_test_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Learning rate scheduler 설정\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "best_model.train()\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    train_loss = train(train_loader, optimizer)\n",
    "    test_loss = test(test_loader)\n",
    "    \n",
    "    # Learning rate scheduler 업데이트\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Early stopping 조건 검사\n",
    "    if test_loss < best_test_loss - min_delta:\n",
    "        best_test_loss = test_loss\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), 'final_trained_model_weights.pth')\n",
    "\n",
    "with open('final_trained_model_params.json', 'w') as f:\n",
    "    json.dump(model_params, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    " 모델이 처음 보는 유저 또는 도서가 있는 경우, 해당 유저 또는 도서의 노드 특성을 생성하고 기존 그래프 데이터에 추가해야 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./data/prepro_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(test_df['Age'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "test_df['Location_encoded'] = le.fit_transform(test_df['Location'])\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=29)\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(test_df['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()\n",
    "\n",
    "title_embeddings = test_df['Book-Title'].apply(get_title_embedding_fasttext).tolist()\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)\n",
    "\n",
    "test_df['Publisher_encoded'] = le.fit_transform(test_df['Publisher'])\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=10)\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(test_df['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique_user_ids = test_df['User-ID'].unique().tolist()\n",
    "new_unique_book_ids = test_df['Book-ID'].unique().tolist()\n",
    "\n",
    "# 새로운 사용자 ID에 대한 인덱스를 할당합니다.\n",
    "last_used_index = 326696\n",
    "for new_user_id in new_unique_user_ids:\n",
    "    if new_user_id not in UserNodeID_dict:\n",
    "        last_used_index += 1\n",
    "        UserNodeID_dict[new_user_id] = last_used_index\n",
    "\n",
    "# 새로운 도서 ID에 대한 인덱스를 할당합니다.\n",
    "for new_book_id in new_unique_book_ids:\n",
    "    if new_book_id not in BookNodeID_dict:\n",
    "        last_used_index += 1\n",
    "        BookNodeID_dict[new_book_id] = last_used_index\n",
    "        \n",
    "# IDs 매핑 진행\n",
    "test_df['User-ID'] = test_df['User-ID'].map(UserNodeID_dict)\n",
    "test_df['Book-ID'] = test_df['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_unique_node_count = len(UserNodeID_dict) + len(BookNodeID_dict)\n",
    "new_feature_matrix = np.zeros((new_unique_node_count, feature_dim))\n",
    "\n",
    "user_ids = test_df['User-ID'].unique().tolist()  # 유니크 처리함\n",
    "book_ids = test_df['Book-ID'].unique().tolist()\n",
    "\n",
    "# user_id와 book_id는 이미 정수로 매핑되어 있다고 가정\n",
    "# 스케일링을 개별적으로 수행\n",
    "age_tensor_scaled = scaler.fit_transform(age_tensor.reshape(-1, 1))\n",
    "location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor_scaled, location_embeddings_scaled):\n",
    "    new_feature_matrix[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "title_embeddings_scaled = scaler.fit_transform(reduced_title_embeddings)\n",
    "publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, title_embeddings_scaled, publisher_embeddings_scaled):\n",
    "    new_feature_matrix[book_id - num_user_nodes] = np.concatenate([title, publisher], axis=0)\n",
    "\n",
    "new_node_feature_matrix = torch.tensor(new_feature_matrix, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_trained_model_params.json', 'r') as f:\n",
    "    loaded_model_params = json.load(f)\n",
    "    \n",
    "final_model = GraphSAGERegressor(**loaded_model_params).to(device)\n",
    "\n",
    "final_model.load_state_dict(torch.load('final_trained_model_weights.pth'))\n",
    "\n",
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 불러오기\n",
    "loaded_weights = torch.load('final_trained_model_weights.pth')\n",
    "\n",
    "# best_model의 상태 사전 출력\n",
    "print(\"final_model's state_dict:\")\n",
    "for param_tensor in final_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", best_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"\\nLoaded weights:\")\n",
    "for param_tensor in loaded_weights:\n",
    "    print(param_tensor, \"\\t\", loaded_weights[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_edge_index = torch.tensor(test_df[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()\n",
    "new_data = Data(x=torch.tensor(new_feature_matrix, dtype=torch.float), edge_index=new_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_loader = DataLoader([new_data], batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for batch in new_data_loader:\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = final_model(batch.x, [batch.edge_index])\n",
    "        edge_index_row, edge_index_col = batch.edge_index\n",
    "        edge_predictions = out[edge_index_row] * out[edge_index_col]\n",
    "        predictions.append(edge_predictions.sum(dim=-1).cpu().numpy())\n",
    "\n",
    "# Combine the predictions\n",
    "predictions = np.concatenate(predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
