{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader, Batch\n",
    "from torch_geometric.nn import SAGEConv, GAE\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Embedding\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./prepro_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Graph\n",
    "## Category Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(train_data['Age'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"age_tensor.shape >>> \",age_tensor.shape)\n",
    "print(age_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Location\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['Location_encoded'] = le.fit_transform(train_data['Location'])\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=29)\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(train_data['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"location_embeddings.shape >>> \",location_embeddings.shape)\n",
    "print(location_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book-Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# # 사전 훈련된 FastText 모델 다운로드\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "# !gunzip cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_embedding_fasttext(title):\n",
    "    words = title.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    embeddings = [fasttext_model.get_word_vector(word) for word in words]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = train_data['Book-Title'].apply(get_title_embedding_fasttext).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_titles_count = 0\n",
    "for title in train_data['Book-Title']:\n",
    "    words = title.split()\n",
    "    if not words:\n",
    "        empty_titles_count += 1\n",
    "\n",
    "print(f\"빈 문자열이거나 토큰화된 단어가 없는 책 제목의 개수: {empty_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, emb in enumerate(title_embeddings):\n",
    "    if not isinstance(emb, np.ndarray) or emb.shape != (300,):\n",
    "        print(f\"Index: {idx}, Title: {train_data['Book-Title'][idx]}, Embedding: {emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reduced_title_embeddings.shape >>> \",reduced_title_embeddings.shape)\n",
    "print(reduced_title_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Publisher\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['Publisher_encoded'] = le.fit_transform(train_data['Publisher'])\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=10)\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(train_data['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Publisher_embeddings.shape >>> \",Publisher_embeddings.shape)\n",
    "print(Publisher_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-ID & Book-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "# User-ID 열의 unique한 값들을 리스트로 만들기  > 83256\n",
    "unique_user_ids = train_data['User-ID'].unique().tolist()\n",
    "# Book-ID 열의 unique한 값들을 리스트로 만들기 > 243441\n",
    "unique_book_ids = train_data['Book-ID'].unique().tolist()\n",
    "\n",
    "# unique_user_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_user_ids_df = unique_user_ids_df.sort_values(by='User-ID', ascending=True)\n",
    "# # 인덱스를 새 column으로 추가\n",
    "sorted_unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_user_ids_df.rename(columns={'index': 'UserNodeID'}, inplace=True)\n",
    "\n",
    "# unique_book_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_book_ids_df = unique_book_ids_df.sort_values(by='Book-ID', ascending=True)\n",
    "# #인덱스를 새 column으로 추가\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_book_ids_df.rename(columns={'index': 'BookNodeID'}, inplace=True)\n",
    "\n",
    "# 인덱스 시작 번호를 83256으로 설정\n",
    "sorted_unique_book_ids_df['BookNodeID'] += 83256\n",
    "\n",
    "# UserNodeID  0 ~ 83255\n",
    "# BookNodeID   83256 ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ID_dict(df):\n",
    "    ID_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        ID_dict[row[1]] = row[0]\n",
    "    return ID_dict\n",
    "\n",
    "UserNodeID_dict = make_ID_dict(sorted_unique_user_ids_df)\n",
    "BookNodeID_dict = make_ID_dict(sorted_unique_book_ids_df)\n",
    "\n",
    "####### 매핑 진행\n",
    "train_data['User-ID'] = train_data['User-ID'].map(UserNodeID_dict)\n",
    "train_data['Book-ID'] = train_data['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(train_data[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 노드와 도서 노드의 총 수 계산:\n",
    "\n",
    "num_user_nodes = len(unique_user_ids)\n",
    "num_book_nodes = len(unique_book_ids)\n",
    "\n",
    "user_ids = train_data[\"User-ID\"].values.tolist()\n",
    "book_ids = train_data[\"Book-ID\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_dim = 30  # age (1) + location (29)\n",
    "book_feature_dim = 30  # title (20) + publisher (10)\n",
    "\n",
    "user_features = np.zeros((num_user_nodes, user_feature_dim))\n",
    "book_features = np.zeros((num_book_nodes, book_feature_dim))\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor, location_embeddings):\n",
    "    user_features[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, reduced_title_embeddings, publisher_embeddings):\n",
    "    book_features[book_id - num_user_nodes] = np.concatenate([title, publisher], axis=0)\n",
    "\n",
    "node_features = np.vstack((user_features, book_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
