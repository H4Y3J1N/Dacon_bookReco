{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader, Batch\n",
    "from torch_geometric.nn import SAGEConv, GAE\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Embedding\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./prepro_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Graph\n",
    "## Category Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(train_data['Age'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"age_tensor.shape >>> \",age_tensor.shape)\n",
    "print(age_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Location\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['Location_encoded'] = le.fit_transform(train_data['Location'])\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=29)\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(train_data['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"location_embeddings.shape >>> \",location_embeddings.shape)\n",
    "print(location_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book-Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# # 사전 훈련된 FastText 모델 다운로드\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "# !gunzip cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_embedding_fasttext(title):\n",
    "    words = title.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    embeddings = [fasttext_model.get_word_vector(word) for word in words]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = train_data['Book-Title'].apply(get_title_embedding_fasttext).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_titles_count = 0\n",
    "for title in train_data['Book-Title']:\n",
    "    words = title.split()\n",
    "    if not words:\n",
    "        empty_titles_count += 1\n",
    "\n",
    "print(f\"빈 문자열이거나 토큰화된 단어가 없는 책 제목의 개수: {empty_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, emb in enumerate(title_embeddings):\n",
    "    if not isinstance(emb, np.ndarray) or emb.shape != (300,):\n",
    "        print(f\"Index: {idx}, Title: {train_data['Book-Title'][idx]}, Embedding: {emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reduced_title_embeddings.shape >>> \",reduced_title_embeddings.shape)\n",
    "print(reduced_title_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Publisher\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['Publisher_encoded'] = le.fit_transform(train_data['Publisher'])\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=10)\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(train_data['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Publisher_embeddings.shape >>> \",Publisher_embeddings.shape)\n",
    "print(Publisher_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-ID & Book-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "# User-ID 열의 unique한 값들을 리스트로 만들기  > 83256\n",
    "unique_user_ids = train_data['User-ID'].unique().tolist()\n",
    "# Book-ID 열의 unique한 값들을 리스트로 만들기 > 243441\n",
    "unique_book_ids = train_data['Book-ID'].unique().tolist()\n",
    "\n",
    "# unique_user_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_user_ids_df = unique_user_ids_df.sort_values(by='User-ID', ascending=True)\n",
    "# # 인덱스를 새 column으로 추가\n",
    "sorted_unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_user_ids_df.rename(columns={'index': 'UserNodeID'}, inplace=True)\n",
    "\n",
    "# unique_book_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_book_ids_df = unique_book_ids_df.sort_values(by='Book-ID', ascending=True)\n",
    "# #인덱스를 새 column으로 추가\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_book_ids_df.rename(columns={'index': 'BookNodeID'}, inplace=True)\n",
    "\n",
    "# 인덱스 시작 번호를 83256으로 설정\n",
    "sorted_unique_book_ids_df['BookNodeID'] += 83256\n",
    "\n",
    "# UserNodeID  0 ~ 83255\n",
    "# BookNodeID   83256 ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ID_dict(df):\n",
    "    ID_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        ID_dict[row[1]] = row[0]\n",
    "    return ID_dict\n",
    "\n",
    "UserNodeID_dict = make_ID_dict(sorted_unique_user_ids_df)\n",
    "BookNodeID_dict = make_ID_dict(sorted_unique_book_ids_df)\n",
    "\n",
    "####### 매핑 진행\n",
    "train_data['User-ID'] = train_data['User-ID'].map(UserNodeID_dict)\n",
    "train_data['Book-ID'] = train_data['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(train_data[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 노드와 도서 노드의 총 수 계산:\n",
    "\n",
    "num_user_nodes = len(unique_user_ids)\n",
    "num_book_nodes = len(unique_book_ids)\n",
    "\n",
    "user_ids = train_data[\"User-ID\"].values.tolist()\n",
    "book_ids = train_data[\"Book-ID\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_dim = 30  # age (1) + location (29)\n",
    "book_feature_dim = 30  # title (20) + publisher (10)\n",
    "\n",
    "user_features = np.zeros((num_user_nodes, user_feature_dim))\n",
    "book_features = np.zeros((num_book_nodes, book_feature_dim))\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor, location_embeddings):\n",
    "    user_features[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, reduced_title_embeddings, publisher_embeddings):\n",
    "    book_features[book_id - num_user_nodes] = np.concatenate([title, publisher], axis=0)\n",
    "\n",
    "node_features = np.vstack((user_features, book_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = torch.tensor(train_data['Book-Rating'].values, dtype=torch.float).unsqueeze(-1)\n",
    "y = edge_attr.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = Data(x=torch.tensor(node_features), edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "graph_data.n_id = torch.arange(graph_data.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Split - Node base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code\n",
    "\n",
    "num_user_nodes = len(unique_user_ids)\n",
    "num_book_nodes = len(unique_book_ids)\n",
    "\n",
    "user_item_matrix = np.zeros((num_user_nodes, num_book_nodes))\n",
    "\n",
    "for edge_index, edge_attr in zip(graph_data.edge_index.t(), graph_data.edge_attr):\n",
    "    user, item = edge_index\n",
    "    rating = edge_attr\n",
    "    user_item_matrix[user, item] = rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "edges = np.argwhere(user_item_matrix > 0)\n",
    "ratings = user_item_matrix[edges[:, 0], edges[:, 1]]\n",
    "\n",
    "train_indices, test_indices, _, _ = train_test_split(\n",
    "    np.arange(edges.shape[0]), ratings, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_edges, test_edges = edges[train_indices], edges[test_indices]\n",
    "train_ratings, test_ratings = ratings[train_indices], ratings[test_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = Data(\n",
    "    x=graph_data.x, edge_index=torch.tensor(train_edges).t().contiguous(), edge_attr=torch.tensor(train_ratings)\n",
    ")\n",
    "test_data = Data(\n",
    "    x=graph_data.x, edge_index=torch.tensor(test_edges).t().contiguous(), edge_attr=torch.tensor(test_ratings)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class WeightedSAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(WeightedSAGEConv, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        x = self.lin(x)\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return x_j * edge_attr.view(-1, 1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGERegressor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(GraphSAGERegressor, self).__init__()\n",
    "        self.sage1 = WeightedSAGEConv(in_channels, hidden_channels)\n",
    "        self.sage2 = WeightedSAGEConv(hidden_channels, hidden_channels)\n",
    "        self.sage3 = WeightedSAGEConv(hidden_channels, out_channels)\n",
    "        self.batch_norm1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.batch_norm2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = F.relu(self.sage1(x, edge_index, edge_attr))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = F.relu(self.sage2(x, edge_index, edge_attr))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.sage3(x, edge_index, edge_attr)\n",
    "        return x.squeeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "train_data = train_data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "in_channels = 30\n",
    "hidden_channels = 64\n",
    "out_channels = 1\n",
    "\n",
    "model = GraphSAGERegressor(in_channels, hidden_channels, out_channels).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr)\n",
    "        \n",
    "        predictions = out[data.edge_index[0], data.edge_index[1]]\n",
    "        loss = criterion(predictions, data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "            predictions = out[data.edge_index[0], data.edge_index[1]]\n",
    "            loss = criterion(predictions, data.y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = 10\n",
    "best_test_loss = float(\"inf\")\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train_loss = train(train_loader)\n",
    "    test_loss = test(test_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}')\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 튜닝을 위해 베이지안 최적화 라이브러리인 optuna를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 추천값 설정\n",
    "    hidden_channels = trial.suggest_int('hidden_channels', 32, 128)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # 모델 및 최적화 생성\n",
    "    model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # 학습 및 평가 루프\n",
    "    best_test_loss = float('inf')\n",
    "    for epoch in range(1, 201):\n",
    "        train_loss = train(train_loader)\n",
    "        test_loss = test(test_loader)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "\n",
    "    return best_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=15)  # n_trials는 원하는 튜닝 횟수에 따라 조정할 수 있습니다.\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: {best_trial.number}, Test Loss: {best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    " 모델이 처음 보는 유저 또는 도서가 있는 경우, 해당 유저 또는 도서의 노드 특성을 생성하고 기존 그래프 데이터에 추가해야 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(new_user_book_pairs, user_id_to_index, book_id_to_index):\n",
    "    model.eval()\n",
    "    predicted_ratings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_id, book_id in new_user_book_pairs:\n",
    "            user_index = user_id_to_index[user_id]\n",
    "            book_index = book_id_to_index[book_id]\n",
    "\n",
    "            user_tensor = torch.tensor([user_index], dtype=torch.long, device=device)\n",
    "            book_tensor = torch.tensor([book_index], dtype=torch.long, device=device)\n",
    "\n",
    "            edge_index = torch.stack([user_tensor, book_tensor], dim=0)\n",
    "            edge_attr = torch.tensor([1], dtype=torch.float, device=device)\n",
    "\n",
    "            out = model(graph_data.x.to(device), edge_index, edge_attr)\n",
    "            rating = out[user_index, book_index].item()\n",
    "\n",
    "            predicted_ratings.append((user_id, book_id, rating))\n",
    "\n",
    "    return predicted_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = predict_ratings(new_user_book_pairs, user_id_to_index, book_id_to_index)\n",
    "\n",
    "print(predicted_ratings)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
