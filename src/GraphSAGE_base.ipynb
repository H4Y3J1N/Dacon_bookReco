{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch.nn as nn\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, NeighborSampler, Batch\n",
    "from torch_geometric.nn import SAGEConv, GAE, TopKPooling\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn import Embedding\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gensim.downloader as api\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./prepro_train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Graph\n",
    "## Category Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_tensor = torch.tensor(dataset['Age'].values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"age_tensor.shape >>> \",age_tensor.shape)\n",
    "print(age_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Location\n",
    "\n",
    "le = LabelEncoder()\n",
    "dataset['Location_encoded'] = le.fit_transform(dataset['Location'])\n",
    "embedding_layer = Embedding(num_embeddings=151, embedding_dim=29)\n",
    "location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "location_embeddings = location_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"location_embeddings.shape >>> \",location_embeddings.shape)\n",
    "print(location_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Book-Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# # 사전 훈련된 FastText 모델 다운로드\n",
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
    "# !gunzip cc.en.300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_embedding_fasttext(title):\n",
    "    words = title.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    embeddings = [fasttext_model.get_word_vector(word) for word in words]\n",
    "    return np.mean(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_embeddings = dataset['Book-Title'].apply(get_title_embedding_fasttext).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_titles_count = 0\n",
    "for title in dataset['Book-Title']:\n",
    "    words = title.split()\n",
    "    if not words:\n",
    "        empty_titles_count += 1\n",
    "\n",
    "print(f\"빈 문자열이거나 토큰화된 단어가 없는 책 제목의 개수: {empty_titles_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, emb in enumerate(title_embeddings):\n",
    "    if not isinstance(emb, np.ndarray) or emb.shape != (300,):\n",
    "        print(f\"Index: {idx}, Title: {dataset['Book-Title'][idx]}, Embedding: {emb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "title_embeddings_array = np.array(title_embeddings)\n",
    "reduced_title_embeddings = pca.fit_transform(title_embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"reduced_title_embeddings.shape >>> \",reduced_title_embeddings.shape)\n",
    "print(reduced_title_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Publisher\n",
    "\n",
    "le = LabelEncoder()\n",
    "dataset['Publisher_encoded'] = le.fit_transform(dataset['Publisher'])\n",
    "embedding_layer = Embedding(num_embeddings=3689, embedding_dim=10)\n",
    "publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Publisher_embeddings.shape >>> \",Publisher_embeddings.shape)\n",
    "print(Publisher_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-ID & Book-ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "# User-ID 열의 unique한 값들을 리스트로 만들기  > 83256\n",
    "unique_user_ids = dataset['User-ID'].unique().tolist()\n",
    "# Book-ID 열의 unique한 값들을 리스트로 만들기 > 243441\n",
    "unique_book_ids = dataset['Book-ID'].unique().tolist()\n",
    "\n",
    "# unique_user_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_user_ids_df = unique_user_ids_df.sort_values(by='User-ID', ascending=True)\n",
    "# # 인덱스를 새 column으로 추가\n",
    "sorted_unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_user_ids_df.rename(columns={'index': 'UserNodeID'}, inplace=True)\n",
    "\n",
    "# unique_book_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_book_ids_df = unique_book_ids_df.sort_values(by='Book-ID', ascending=True)\n",
    "# #인덱스를 새 column으로 추가\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_book_ids_df.rename(columns={'index': 'BookNodeID'}, inplace=True)\n",
    "\n",
    "# 인덱스 시작 번호를 83256으로 설정\n",
    "sorted_unique_book_ids_df['BookNodeID'] += 83256\n",
    "\n",
    "# UserNodeID  0 ~ 83255\n",
    "# BookNodeID   83256 ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ID_dict(df):\n",
    "    ID_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        ID_dict[row[1]] = row[0]\n",
    "    return ID_dict\n",
    "\n",
    "UserNodeID_dict = make_ID_dict(sorted_unique_user_ids_df)\n",
    "BookNodeID_dict = make_ID_dict(sorted_unique_book_ids_df)\n",
    "\n",
    "####### 매핑 진행\n",
    "dataset['User-ID'] = dataset['User-ID'].map(UserNodeID_dict)\n",
    "dataset['Book-ID'] = dataset['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(dataset[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 노드와 도서 노드의 총 수 계산:\n",
    "\n",
    "num_user_nodes = len(unique_user_ids)\n",
    "num_book_nodes = len(unique_book_ids)\n",
    "\n",
    "user_ids = dataset[\"User-ID\"].values.tolist()\n",
    "book_ids = dataset[\"Book-ID\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_feature_dim = 30  # age (1) + location (29)\n",
    "book_feature_dim = 30  # title (20) + publisher (10)\n",
    "\n",
    "user_features = np.zeros((num_user_nodes, user_feature_dim))\n",
    "book_features = np.zeros((num_book_nodes, book_feature_dim))\n",
    "\n",
    "for user_id, age, location in zip(user_ids, age_tensor, location_embeddings):\n",
    "    user_features[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "for book_id, title, publisher in zip(book_ids, reduced_title_embeddings, publisher_embeddings):\n",
    "    book_features[book_id - num_user_nodes] = np.concatenate([title, publisher], axis=0)\n",
    "\n",
    "node_features = np.vstack((user_features, book_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = torch.tensor(dataset['Book-Rating'].values, dtype=torch.float).unsqueeze(-1)\n",
    "y = edge_attr.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = Data(x=torch.tensor(node_features), edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "graph_data.n_id = torch.arange(graph_data.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Split - Node base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분할\n",
    "total_edges = edge_index.shape[1]\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_idx = train_data.index.values\n",
    "test_idx = test_data.index.values\n",
    "\n",
    "# train/test 마스크 초기화\n",
    "train_mask = torch.zeros(total_edges, dtype=bool)\n",
    "test_mask = torch.zeros(total_edges, dtype=bool)\n",
    "\n",
    "# train/test 마스크 설정\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# 그래프 데이터 생성\n",
    "data = Data(x=torch.tensor(node_features, dtype=torch.float),\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=y,\n",
    "            train_mask=train_mask,\n",
    "            test_mask=test_mask)\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "# Train/test 마스크가 잘 설정되었는지 확인\n",
    "print(data.train_mask.sum())  # 결과는 전체 데이터의 약 80%여야 합니다. tensor(697114)\n",
    "print(data.test_mask.sum())   # 결과는 전체 데이터의 약 20%여야 합니다.tensor(174279)\n",
    "\n",
    "# UserNodeID  0 ~ 83255\n",
    "# BookNodeID   83256 ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx_tensor = torch.tensor(train_idx)\n",
    "test_idx_tensor = torch.tensor(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.num_nodes = max(data.edge_index.max().item(), len(data.x)) + 1\n",
    "\n",
    "train_loader = NeighborSampler(data.edge_index, node_idx=train_idx_tensor, sizes=[5, 3], batch_size=2, shuffle=True, num_nodes=data.num_nodes)\n",
    "test_loader = NeighborSampler(data.edge_index, node_idx=test_idx_tensor, sizes=[5, 3], batch_size=2, shuffle=False, num_nodes=data.num_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class WeightedSAGEConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(WeightedSAGEConv, self).__init__(aggr='mean')\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        edge_attr = edge_attr[col]  # 인접 노드에 대한 edge_attr만 선택\n",
    "        edge_index, edge_attr = add_self_loops(edge_index, edge_attr, num_nodes=x.size(0))\n",
    "        x = self.lin(x)\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        return x_j * edge_attr.view(-1, 1)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GraphSAGERegressor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout, activation_name, num_layers=2):\n",
    "        super(GraphSAGERegressor, self).__init__()\n",
    "\n",
    "        # Select the activation function\n",
    "        activations = torch.nn.ModuleDict([\n",
    "            ['relu', torch.nn.ReLU()],\n",
    "            ['leaky_relu', torch.nn.LeakyReLU()],\n",
    "            ['prelu', torch.nn.PReLU()],\n",
    "            ['elu', torch.nn.ELU()],\n",
    "            ['silu', torch.nn.SiLU()]\n",
    "        ])\n",
    "        self.activation = activations[activation_name]\n",
    "        if self.activation is None:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_name}\")\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        # Add the first layer\n",
    "        self.convs.append(WeightedSAGEConv(in_channels, hidden_channels))\n",
    "        self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Add intermediate layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(WeightedSAGEConv(hidden_channels, hidden_channels))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_channels))\n",
    "\n",
    "        # Add the last layer\n",
    "        self.convs.append(WeightedSAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, edge_index_list, edge_attr_list):\n",
    "        \n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x, edge_index_list[i], edge_attr_list[i])\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        x = self.convs[-1](x, edge_index_list[-1], edge_attr_list[-1])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_channels = 30\n",
    "hidden_channels = 64\n",
    "out_channels = 1\n",
    "activation_name = 'relu'\n",
    "dropout =  0.1\n",
    "\n",
    "model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout, activation_name, num_layers=2)\n",
    "model = model.to(device)  # 모델만 옮기고, 데이터는 옮기지 않는다. 그래야 메모리 효율성 업!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        edge_index_list = [adj.edge_index.to(device) for adj in adjs]\n",
    "        edge_attr_list = [data.edge_attr.to(device) for _ in adjs]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "        predictions = out[:batch_size]\n",
    "        loss = criterion(predictions, data.y[n_id[data.train_mask][:batch_size]].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_size, n_id, adjs in test_loader:\n",
    "            edge_index_list = [adj.edge_index.to(device) for adj in adjs]\n",
    "            edge_attr_list = [data.edge_attr.to(device) for _ in adjs]\n",
    "            \n",
    "            out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "            predictions = out[:batch_size]\n",
    "            loss = criterion(predictions, data.y[n_id[data.test_mask][:batch_size]].to(device))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_channels = 30\n",
    "hidden_channels = 32\n",
    "out_channels = 1\n",
    "activation_name = 'relu'\n",
    "dropout =  0.1\n",
    "\n",
    "model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout, activation_name, num_layers=2)\n",
    "model = model.to(device)  # 모델만 옮기고, 데이터는 옮기지 않는다. 그래야 메모리 효율성 업!\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in train_loader:\n",
    "        torch.cuda.empty_cache()\n",
    "        adjs = [adj.to(device) for adj in adjs]  # Move adjs to device\n",
    "        edge_index_list = [adj.edge_index for adj in adjs]\n",
    "        edge_attr_list = [data.edge_attr.to(device) for _ in adjs]  # Reuse edge_attr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "        predictions = out[:batch_size]\n",
    "        loss = criterion(predictions, data.y[n_id[:batch_size]].to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch_size, n_id, adjs in test_loader:\n",
    "        adjs = [adj.to(device) for adj in adjs]  # Move adjs to device\n",
    "        edge_index_list = [adj.edge_index for adj in adjs]\n",
    "        edge_attr_list = [data.edge_attr.to(device) for _ in adjs]  # Reuse edge_attr\n",
    "\n",
    "        out = model(data.x[n_id].to(device), edge_index_list, edge_attr_list)\n",
    "        predictions = out[:batch_size]\n",
    "        loss = criterion(predictions, data.y[n_id[:batch_size]].to(device))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "best_epoch = 0\n",
    "patience = 2\n",
    "counter = 0\n",
    "epoch_num = 10\n",
    "\n",
    "for epoch in tqdm(range(1, epoch_num+1)):\n",
    "    train_loss = train(train_loader)\n",
    "    print(f'Epoch {epoch:02d}, Train Loss: {train_loss:.4f}')\n",
    "    scheduler.step()\n",
    "    \n",
    "    test_loss = test(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "    # Early Stopping\n",
    "    if test_loss < best_val_loss:\n",
    "        best_val_loss = test_loss\n",
    "        best_epoch = epoch\n",
    "        best_model = model.state_dict()\n",
    "        torch.save(best_model, 'best_model.pt')  # Save the best model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping: validation loss did not improve for {patience} epochs.')\n",
    "            break\n",
    "\n",
    "print(f'Best validation loss of {best_val_loss:.4f} was achieved at epoch {best_epoch}.')\n",
    "best_model = torch.load('best_model.pt')  # Load the best model\n",
    "model.load_state_dict(best_model)\n",
    "test_loss = test(test_loader)\n",
    "print(f'Test loss of {test_loss:.4f} was achieved at epoch {best_epoch}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 튜닝을 위해 베이지안 최적화 라이브러리인 optuna를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 추천값 설정\n",
    "    hidden_channels = trial.suggest_int('hidden_channels', 32, 64, 128)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    activation_name = trial.suggest_categorical('activation_name', ['relu', 'leaky_relu', 'prelu', 'elu', 'silu'])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer_name', ['Adam', 'AdamW', 'RMSprop', 'Adagrad'])\n",
    "\n",
    "    # 모델 및 최적화 생성\n",
    "    model = GraphSAGERegressor(in_channels, hidden_channels, out_channels, dropout, activation_name).to(device)\n",
    "\n",
    "    optimizer_class = getattr(torch.optim, optimizer_name)\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # DataLoader 수정\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 학습 및 평가 루프\n",
    "    best_test_loss = float('inf')\n",
    "    for epoch in range(1, 201):\n",
    "        train_loss = train(train_loader)\n",
    "        test_loss = test(test_loader)\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "\n",
    "    return best_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=15)  # n_trials는 원하는 튜닝 횟수에 따라 조정할 수 있습니다.\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(f\"Best trial: {best_trial.number}, Test Loss: {best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Best Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_patience = 10\n",
    "best_test_loss = float(\"inf\")\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    train_loss = train(train_loader)\n",
    "    test_loss = test(test_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}')\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    " 모델이 처음 보는 유저 또는 도서가 있는 경우, 해당 유저 또는 도서의 노드 특성을 생성하고 기존 그래프 데이터에 추가해야 함!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ratings(new_user_book_pairs, user_id_to_index, book_id_to_index):\n",
    "    model.eval()\n",
    "    predicted_ratings = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for user_id, book_id in new_user_book_pairs:\n",
    "            user_index = user_id_to_index[user_id]\n",
    "            book_index = book_id_to_index[book_id]\n",
    "\n",
    "            user_tensor = torch.tensor([user_index], dtype=torch.long, device=device)\n",
    "            book_tensor = torch.tensor([book_index], dtype=torch.long, device=device)\n",
    "\n",
    "            edge_index = torch.stack([user_tensor, book_tensor], dim=0)\n",
    "            edge_attr = torch.tensor([1], dtype=torch.float, device=device)\n",
    "\n",
    "            out = model(graph_data.x.to(device), edge_index, edge_attr)\n",
    "            rating = out[user_index, book_index].item()\n",
    "\n",
    "            predicted_ratings.append((user_id, book_id, rating))\n",
    "\n",
    "    return predicted_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = predict_ratings(new_user_book_pairs, user_id_to_index, book_id_to_index)\n",
    "\n",
    "print(predicted_ratings)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
