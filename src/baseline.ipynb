{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Dataset, DataLoader\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import SAGEConv, GAE\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## 데이터 전처리\n",
    "################## 전처리 미진행 column : Location, Book-Title, Book-Author, Publisher \n",
    "\n",
    "users = data[['User-ID', 'Age']].drop_duplicates().reset_index(drop=True)\n",
    "books = data[['Book-ID', 'Year-Of-Publication']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "user_id_map = {user_id: idx for idx, user_id in enumerate(users['User-ID'].unique())}\n",
    "book_id_map = {book_id: idx for idx, book_id in enumerate(books['Book-ID'].unique())}\n",
    "users['User-ID'] = users['User-ID'].map(user_id_map)\n",
    "books['Book-ID'] = books['Book-ID'].map(book_id_map)\n",
    "\n",
    "data['Age'] = (users['Age'] - users['Age'].min()) / (users['Age'].max() - users['Age'].min())\n",
    "data['Year-Of-Publication'] = (books['Year-Of-Publication'] - books['Year-Of-Publication'].min()) / (books['Year-Of-Publication'].max() - books['Year-Of-Publication'].min())\n",
    "\n",
    "data['User-ID'] = data['User-ID'].map(user_id_map)\n",
    "data['Book-ID'] = data['Book-ID'].map(book_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed = data.copy()\n",
    "data_processed = data_processed.fillna(0)\n",
    "\n",
    "node_features = torch.tensor(pd.concat([users['Age'], books['Year-Of-Publication']]).values, dtype=torch.float).unsqueeze(1)\n",
    "edge_index = torch.tensor(data_processed[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "target = torch.tensor(data_processed['Book-Rating'].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_index)\n",
    "print(edge_index.shape)\n",
    "print(f\"Number of rows in data_processed: {len(data_processed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BookRatingData(Dataset):\n",
    "    def __init__(self, node_features, edge_index, target):\n",
    "        super(BookRatingData, self).__init__()\n",
    "        self.node_features = node_features\n",
    "        self.edge_index = edge_index\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.edge_index.size(1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x': self.node_features,\n",
    "            'edge_index': self.edge_index[:, idx],\n",
    "            'y': self.target[idx]\n",
    "        }\n",
    "\n",
    "# 원본 데이터셋 생성\n",
    "book_rating_data = BookRatingData(node_features, edge_index, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(book_rating_data.edge_index)\n",
    "print(book_rating_data.edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges(edge_index, split_ratio=(0.8, 0.2)):\n",
    "    num_edges = edge_index.size(1)\n",
    "    indices = list(range(num_edges))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    split_idx = int(split_ratio[0] * num_edges)\n",
    "    train_indices, test_indices = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "    train_edge_index = edge_index[:, train_indices]\n",
    "    test_edge_index = edge_index[:, test_indices]\n",
    "#     train_edge_attr = edge_attr[train_indices]\n",
    "#     test_edge_attr = edge_attr[test_indices]\n",
    "\n",
    "    return train_edge_index, test_edge_index\n",
    "\n",
    "# 무작위로 엣지를 분할합니다.\n",
    "train_dataset, test_dataset = split_edges(book_rating_data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGERatingPredictor(nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_layers, dropout):\n",
    "        super(GraphSAGERatingPredictor, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(SAGEConv(num_features, hidden_channels))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.layers.append(SAGEConv(hidden_channels, 1))\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.layers[-1](x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        y = data.y\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, edge_index)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            x = data.x\n",
    "            edge_index = data.edge_index\n",
    "            y = data.y\n",
    "            \n",
    "            out = model(x, edge_index)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = node_features.size(1)\n",
    "hidden_channels = 64\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "epochs = 15\n",
    "lr = 0.01\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphSAGERatingPredictor(num_features, hidden_channels, num_layers, dropout).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss = test(model, test_loader, criterion)\n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
