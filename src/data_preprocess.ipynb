{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Age, Year-Of-Publication\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_to_scale = ['Age', 'Year-Of-Publication']\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "train_data[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Location  // nunique() > 442 > mapping : 151\n",
    "\n",
    "train_data['Location'] = train_data['Location'].apply(lambda x: x.split(', ')[-1].strip())\n",
    "\n",
    "with open(\"country_name_mapping.pkl\", \"rb\") as f:\n",
    "    country_name_mapping = pickle.load(f)\n",
    "    \n",
    "train_data['Location'] = train_data['Location'].apply(lambda x: country_name_mapping.get(x, x))\n",
    "    \n",
    "le = LabelEncoder()\n",
    "train_data['Location_encoded'] = le.fit_transform(train_data['Location'])\n",
    "\n",
    "embedding_layer = nn.Embedding(num_embeddings=151, embedding_dim=25)\n",
    "\n",
    "location_embeddings = embedding_layer(torch.arange(151))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_embeddings_matrix = np.zeros((num_nodes, 25))\n",
    "\n",
    "for i, location_encoded in enumerate(train_data['Location_encoded']):\n",
    "    location_embeddings_matrix[i] = location_embeddings[location_encoded].numpy()\n",
    "\n",
    "X_new = np.concatenate([X, location_embeddings_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### User-ID & Book-ID\n",
    "\n",
    "# User-ID 열의 unique한 값들을 리스트로 만들기  > 83256\n",
    "unique_user_ids = train_data['User-ID'].unique().tolist()\n",
    "\n",
    "# Book-ID 열의 unique한 값들을 리스트로 만들기 > 243441> Q. 그럼 unique한 book name도 숫자가 같아야지?\n",
    "unique_book_ids = train_data['Book-ID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_user_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_user_ids_df = unique_user_ids_df.sort_values(by='User-ID', ascending=True)\n",
    "# # 인덱스를 새 column으로 추가\n",
    "sorted_unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_user_ids_df.rename(columns={'index': 'UserNodeID'}, inplace=True)\n",
    "\n",
    "# unique_book_ids 리스트를 기반으로 DataFrame 생성\n",
    "unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID'])\n",
    "# 내림차순 정렬\n",
    "sorted_unique_book_ids_df = unique_book_ids_df.sort_values(by='Book-ID', ascending=True)\n",
    "# #인덱스를 새 column으로 추가\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "sorted_unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "sorted_unique_book_ids_df.rename(columns={'index': 'BookNodeID'}, inplace=True)\n",
    "\n",
    "# 인덱스 시작 번호를 83256으로 설정\n",
    "sorted_unique_book_ids_df['BookNodeID'] += 83256\n",
    "\n",
    "# UserNodeID  0 ~ 83255\n",
    "# BookNodeID   83256 ~ 326696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ID_dict(df):\n",
    "    ID_dict = {}\n",
    "    for index, row in df.iterrows():\n",
    "        ID_dict[row[1]] = row[0]\n",
    "    return ID_dict\n",
    "\n",
    "UserNodeID_dict = make_ID_dict(sorted_unique_user_ids_df)\n",
    "BookNodeID_dict = make_ID_dict(sorted_unique_book_ids_df)\n",
    "\n",
    "train_map = train_data.copy()\n",
    "\n",
    "####### 매핑 진행\n",
    "train_map['User-ID'] = train_map['User-ID'].map(UserNodeID_dict)\n",
    "train_map['Book-ID'] = train_map['Book-ID'].map(BookNodeID_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(train_map[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# node feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_user_nodes = len(unique_user_ids)\n",
    "num_book_nodes = len(unique_book_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
