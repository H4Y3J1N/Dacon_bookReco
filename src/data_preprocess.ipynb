{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book title\n",
    "전처리 이후 train_data['Book-Title'].nunique() >>> 205946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "df = pd.DataFrame(train_data[[\"Book-ID\",\"Book-Title\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def check_special_chars(text):\n",
    "    return set(re.findall(r\"[{}]\".format(string.punctuation), text))\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r\"[{}]\".format(string.punctuation), \"\", text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "df[\"Book-Title_lower\"] = df[\"Book-Title\"].apply(to_lowercase)\n",
    "df[\"Book-Title_special_chars\"] = df[\"Book-Title_lower\"].apply(check_special_chars)\n",
    "df[\"Book-Title_no_special_chars\"] = df[\"Book-Title_lower\"].apply(remove_special_chars)\n",
    "df[\"Book-Title_no_stopwords\"] = df[\"Book-Title_no_special_chars\"].apply(remove_stopwords)\n",
    "df[\"Book-Title_lemmatized\"] = df[\"Book-Title_no_stopwords\"].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookTitle_prePro_selected = bookTitle_prePro[[\"Book-Title_lemmatized\"]]\n",
    "bookTitle_prePro_selected.rename(columns={'Book-Title_lemmatized': 'Book-Title'}, inplace=True)\n",
    "train_data.update(bookTitle_prePro_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec 임베딩으로 변환. 다만 이 작업은 전략적으로 고려했을 때, Book Node ID로 Feature matrix 정렬이 되고 난 다음이 유리함.\n",
    "* 따라서 여기서 중단!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_data['Location'] = train_data['Location'].apply(lambda x: x.split(', ')[-1].strip())\n",
    "\n",
    "with open(\"country_name_mapping.pkl\", \"rb\") as f:\n",
    "    country_name_mapping = pickle.load(f)\n",
    "    \n",
    "train_data['Location'] = train_data['Location'].apply(lambda x: country_name_mapping.get(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Age, Year-Of-Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features_to_scale = ['Age', 'Year-Of-Publication']\n",
    "scaler = MinMaxScaler()\n",
    "train_data[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book-Author\n",
    "Publisher_list = train_data[\"Book-Author\"].unique().tolist() >>> 92,635     \n",
    "지나치게 sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Publisher'] =train_data['Publisher'].str.replace('[^a-zA-Z0-9\\s]+', '').str.lower()\n",
    "\n",
    "train_data['Publisher'] = train_data['Publisher'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Publisher'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_publishers = train_data['Publisher'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_publishers.head(50)  # 유사한 출판사 이름 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 단어 이상 일치하면 같은 것으로 통합해버리는 map을 만들어 매핑.\n",
    "# difflib 라이브러리를 사용할 수도 있지만, time cost가 너무 크다.\n",
    "\n",
    "def create_publisher_map(publisher_list):\n",
    "    publisher_map = {}\n",
    "    for publisher in publisher_list:\n",
    "        words = publisher.split()\n",
    "        for word in words:\n",
    "            if word not in publisher_map:\n",
    "                publisher_map[word] = publisher\n",
    "            else:\n",
    "                if len(publisher) < len(publisher_map[word]):\n",
    "                    publisher_map[word] = publisher\n",
    "    return publisher_map\n",
    "\n",
    "def map_publisher(publisher, publisher_map):\n",
    "    words = publisher.split()\n",
    "    for word in words:\n",
    "        if word in publisher_map:\n",
    "            return publisher_map[word]\n",
    "    return publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_publishers = train_data['Publisher'].unique()\n",
    "publisher_map = create_publisher_map(unique_publishers)\n",
    "train_data['Publisher_cleaned'] = train_data['Publisher'].apply(lambda x: map_publisher(x, publisher_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Publisher_cleaned'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_counts = train_data['Publisher_cleaned'].value_counts()\n",
    "rare_publishers = publisher_counts[publisher_counts <= 2].index\n",
    "train_data['Publisher_cleaned'] = train_data['Publisher_cleaned'].apply(lambda x: 'Other' if x in rare_publishers else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Publisher_cleaned'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Publisher_cleaned'].value_counts().head(50)  # 정리된 것 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop([\"Publisher\"],axis=1,inplace=True)\n",
    "train_data.rename(columns={'Publisher_cleaned': 'Publisher'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"prepro_train_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
