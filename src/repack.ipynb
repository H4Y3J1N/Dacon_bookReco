{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import_and_load_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn import Embedding\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import tqdm\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.loader import NeighborSampler\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.nn import Embedding\n",
    "\n",
    "def load_data():\n",
    "    dataset = pd.read_csv(\"./data/prepro_train_data.csv\")\n",
    "    test_df = pd.read_csv(\"./data/prepro_test_data.csv\")\n",
    "    submit = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "    return dataset, test_df, submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_preprocessing.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn import Embedding\n",
    "import fasttext\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load FastText model once\n",
    "fasttext_model = fasttext.load_model(\"cc.en.300.bin\")\n",
    "\n",
    "def preprocess_age(dataset):\n",
    "    dataset['Age'].fillna(value=dataset['Age'].mean(), inplace=True)  # handle missing values\n",
    "    age_tensor = torch.tensor(dataset['Age'].values, dtype=torch.float32).unsqueeze(1)\n",
    "    return age_tensor\n",
    "\n",
    "def preprocess_categorical(dataset, column, num_embeddings, embedding_dim):\n",
    "    dataset[column].fillna(value='Missing', inplace=True)  # handle missing values\n",
    "    le = LabelEncoder()\n",
    "    dataset[column + '_encoded'] = le.fit_transform(dataset[column])\n",
    "    embedding_layer = Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "    embeddings = embedding_layer(torch.unsqueeze(torch.tensor(dataset[column + '_encoded'].values, dtype=torch.long), dim=1))\n",
    "    embeddings = embeddings.detach().numpy().squeeze()\n",
    "    return embeddings\n",
    "\n",
    "def get_title_embedding_fasttext(title):\n",
    "    if not isinstance(title, str):\n",
    "        title = \"\"\n",
    "    words = title.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(300)\n",
    "    embeddings = [fasttext_model.get_word_vector(word) for word in words]\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "def preprocess_book_title(dataset):\n",
    "    dataset['Book-Title'].fillna(value='', inplace=True)  # handle missing values\n",
    "    title_embeddings = dataset['Book-Title'].apply(get_title_embedding_fasttext).tolist()\n",
    "    pca = PCA(n_components=50)\n",
    "    title_embeddings_array = np.array(title_embeddings)\n",
    "    reduced_title_embeddings = pca.fit_transform(title_embeddings_array)\n",
    "    return reduced_title_embeddings\n",
    "\n",
    "# Now we can use `preprocess_categorical` function for 'Location' and 'Publisher'\n",
    "def preprocess_location(dataset):\n",
    "    return preprocess_categorical(dataset, 'Location', 151, 79)\n",
    "\n",
    "def preprocess_publisher(dataset):\n",
    "    return preprocess_categorical(dataset, 'Publisher', 3689, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# node_id_mapping.py\n",
    "import pandas as pd\n",
    "\n",
    "def make_id_dict(df, id_column):\n",
    "    return {row[1]: row[0] for row in df.iterrows()}\n",
    "\n",
    "def map_node_ids(dataset, test_df):\n",
    "    unique_user_ids = pd.concat([dataset['User-ID'], test_df['User-ID']]).unique().tolist()\n",
    "    unique_book_ids = pd.concat([dataset['Book-ID'], test_df['Book-ID']]).unique().tolist()\n",
    "\n",
    "    unique_user_ids_df = pd.DataFrame(unique_user_ids, columns=['User-ID']).sort_values(by='User-ID')\n",
    "    unique_user_ids_df.reset_index(inplace=True, drop=False)\n",
    "    unique_user_ids_df.rename(columns={'index': 'user_node_id'}, inplace=True)\n",
    "\n",
    "    unique_book_ids_df = pd.DataFrame(unique_book_ids, columns=['Book-ID']).sort_values(by='Book-ID')\n",
    "    unique_book_ids_df.reset_index(inplace=True, drop=True)\n",
    "    unique_book_ids_df.reset_index(inplace=True, drop=False)\n",
    "    unique_book_ids_df.rename(columns={'index': 'book_node_id'}, inplace=True)\n",
    "    unique_book_ids_df['book_node_id'] += len(unique_user_ids)\n",
    "\n",
    "    user_node_id_dict = make_id_dict(unique_user_ids_df, 'User-ID')\n",
    "    book_node_id_dict = make_id_dict(unique_book_ids_df, 'Book-ID')\n",
    "\n",
    "    dataset['User-ID'] = dataset['User-ID'].map(user_node_id_dict)\n",
    "    dataset['Book-ID'] = dataset['Book-ID'].map(book_node_id_dict)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_feature_matrix.py\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "def create_feature_matrix(dataset, age_tensor, location_embeddings, reduced_title_embeddings, publisher_embeddings):\n",
    "    user_ids = dataset['User-ID'].unique().tolist()\n",
    "    book_ids = dataset['Book-ID'].unique().tolist()\n",
    "\n",
    "    feature_dim = 1 + location_embeddings.shape[1] + reduced_title_embeddings.shape[1] + publisher_embeddings.shape[1]\n",
    "\n",
    "    feature_matrix = np.zeros((len(user_ids) + len(book_ids), feature_dim))\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for user_id, age, location in zip(user_ids, age_tensor, location_embeddings):\n",
    "        feature_matrix[user_id] = np.concatenate([scaler.fit_transform(age.reshape(-1, 1)), scaler.fit_transform(location)], axis=0)\n",
    "\n",
    "    for book_id, title, publisher in zip(book_ids, reduced_title_embeddings, publisher_embeddings):\n",
    "        feature_matrix[book_id] = np.concatenate([scaler.fit_transform(title), scaler.fit_transform(publisher)], axis=0)\n",
    "\n",
    "    node_feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)\n",
    "    edge_attr = torch.tensor(dataset['Book-Rating'].values, dtype=torch.float).unsqueeze(-1)\n",
    "    edge_index = torch.tensor(dataset[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data = Data(x=node_feature_matrix, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "    train_user_ids = np.unique(dataset['User-ID'].values)\n",
    "    train_book_ids = np.unique(dataset['Book-ID'].values)\n",
    "    train_node_ids = np.concatenate((train_user_ids, train_book_ids))\n",
    "\n",
    "    train_idx = torch.tensor(train_node_ids, dtype=torch.long)\n",
    "\n",
    "    return data, train_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphSAGE_model.py\n",
    "import torch\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.py\n",
    "import torch\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from .GraphSAGE_model import GraphSAGE\n",
    "\n",
    "def train(data):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GraphSAGE(data.num_node_features, 128, 64).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    data = data.to(device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(6):\n",
    "        optimizer.zero_grad()\n",
    "        z = model(data.x, data.edge_index)  # Obtain node embeddings\n",
    "\n",
    "        # Negative sampling\n",
    "        edge_index_pos = data.edge_index\n",
    "        edge_index_neg = negative_sampling(edge_index_pos, num_nodes=data.num_nodes,\n",
    "                                           num_neg_samples=edge_index_pos.size(1))\n",
    "\n",
    "        pos_loss = (1 - torch.sigmoid((z[edge_index_pos[0]] * z[edge_index_pos[1]]).sum(dim=-1))).mean()\n",
    "        neg_loss = torch.sigmoid((z[edge_index_neg[0]] * z[edge_index_neg[1]]).sum(dim=-1)).mean()\n",
    "        loss = -torch.log(pos_loss) - torch.log(1 - neg_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_embeddings.py\n",
    "def generate_embeddings(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = data.x.to(torch.device('cuda'))\n",
    "        edge_index = data.edge_index.to(torch.device('cuda'))\n",
    "        embeddings = model(x, edge_index).cpu().detach().numpy()\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_input.py\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_input(dataset, embeddings, edge_index, target):\n",
    "    df = dataset[[\"Age\",\"Location_encoded\",\"Year-Of-Publication\",\"Publisher_encoded\"]]\n",
    "    df_np = df.values\n",
    "    book_title_embeddings = np.stack(dataset[\"Book-Title_encoded\"].values)\n",
    "    edge_index = edge_index.cpu().numpy()\n",
    "    user_embeddings_selected = embeddings[edge_index[0]]\n",
    "    book_embeddings_selected = embeddings[edge_index[1]]\n",
    "    df_emb_list = [df_np, book_title_embeddings, user_embeddings_selected, book_embeddings_selected]\n",
    "    df_emb = np.concatenate(df_emb_list, axis=1)\n",
    "    X = df_emb\n",
    "    y = target.cpu().numpy()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter_optimization.py\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import lightgbm as lgb\n",
    "\n",
    "def hyperparameter_optimization(train_data):\n",
    "    space = {\n",
    "        'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': hp.quniform('bagging_freq', 1, 7, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 5, 10, 1),\n",
    "        'force_col_wise': hp.choice('force_col_wise', [True]),\n",
    "    }\n",
    "\n",
    "    def objective(params):\n",
    "        params = {\n",
    "            'device': 'gpu',\n",
    "            'num_leaves': int(params['num_leaves']),\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'feature_fraction': params['feature_fraction'],\n",
    "            'bagging_fraction': params['bagging_fraction'],\n",
    "            'bagging_freq': int(params['bagging_freq']),\n",
    "            'max_depth': int(params['max_depth']),\n",
    "            'force_col_wise': params['force_col_wise'],\n",
    "            'objective': 'regression',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'verbose': 0\n",
    "        }\n",
    "        cv_results = lgb.cv(params, train_data, num_boost_round=500, nfold=5, \n",
    "                            early_stopping_rounds=50, metrics='rmse', seed=42)\n",
    "        # Hyperopt will try to minimize loss (it always minimizes the objective)\n",
    "        loss = min(cv_results['rmse-mean'])\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "\n",
    "    sorted_trials = sorted(trials.results, key=lambda x: x['loss'])\n",
    "    top5_hyperparameters = [(t['loss'], t['params']) for t in sorted_trials[:5]]\n",
    "\n",
    "    return best, top5_hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_and_save_models.py\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os\n",
    "\n",
    "def train_and_save_models(hyperparameters, train_data, test_data, X_test, y_test):\n",
    "    models = {}\n",
    "    os.makedirs(\"models\", exist_ok=True)  # 모델을 저장할 디렉토리 생성\n",
    "    for i, params in enumerate(hyperparameters):\n",
    "        gbm = lgb.train(params, train_data, num_boost_round=500, valid_sets=test_data,\n",
    "                        early_stopping_rounds=10, verbose_eval=False)\n",
    "        y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "\n",
    "        # Calculate and print metrics\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        print(f\"Model {i} with hyperparameters {params} - RMSE: {rmse:.4f}, MAE: {mae:.4f}, R^2: {r2:.4f}\")\n",
    "\n",
    "        models[i] = gbm\n",
    "        gbm.save_model(f'models/model_{i}.txt')  # 모델을 'models' 디렉토리에 저장\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer_and_submit.py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.data import Data\n",
    "from data_preprocessing import encode_categorical_data, apply_pca_to_embeddings\n",
    "from create_feature_matrix import create_feature_matrix\n",
    "\n",
    "def infer_and_submit(test_df, UserNodeID_dict, BookNodeID_dict, models):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Process age\n",
    "    age_tensor = scaler.fit_transform(test_df['Age'].values.reshape(-1, 1))\n",
    "\n",
    "    # Process location\n",
    "    location_embeddings = encode_categorical_data(test_df, 'Location', 79)\n",
    "    location_embeddings = apply_pca_to_embeddings(location_embeddings, 50)\n",
    "    location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "    # Process book title\n",
    "    title_embeddings = apply_pca_to_embeddings(test_df['Book-Title'].values.tolist(), 50)\n",
    "\n",
    "    # Process publisher\n",
    "    publisher_embeddings = encode_categorical_data(test_df, 'Publisher', 30)\n",
    "    publisher_embeddings = apply_pca_to_embeddings(publisher_embeddings, 30)\n",
    "    publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "\n",
    "    # Prepare features for nodes\n",
    "    test_df['User-ID'] = test_df['User-ID'].map(UserNodeID_dict)\n",
    "    test_df['Book-ID'] = test_df['Book-ID'].map(BookNodeID_dict)\n",
    "\n",
    "    # Create feature matrix using the imported function\n",
    "    feature_matrix, train_idx = create_feature_matrix(test_df, age_tensor, location_embeddings_scaled, title_embeddings, publisher_embeddings_scaled)\n",
    "\n",
    "    # Prepare data for GCN\n",
    "    new_node_feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)\n",
    "    new_edge_index = torch.tensor(test_df[['User-ID', 'Book-ID']].values, dtype=torch.long).t().contiguous().to(device)\n",
    "    new_data = Data(x=new_node_feature_matrix, edge_index=new_edge_index).to(device)\n",
    "\n",
    "    # Get embeddings from GCN\n",
    "    model = train(data)  # You will have to define `train` function or use a pre-trained model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = new_data.x.to(torch.device('cuda'))\n",
    "        edge_index = new_edge_index.to(torch.device('cuda'))\n",
    "        test_embeddings = model(x, new_edge_index).cpu().detach().numpy()\n",
    "\n",
    "    # Prepare features for LGBM\n",
    "    df = test_df[[\"Age\",\"Location_encoded\",\"Year-Of-Publication\",\"Publisher_encoded\"]]\n",
    "    df_np = df.to_numpy()\n",
    "    df_emb = np.concatenate((df_np, test_embeddings), axis=1)\n",
    "\n",
    "    # Make predictions with LGBMs\n",
    "    new_predictions = []\n",
    "    for model in models.values():\n",
    "        pred = model.predict(df_emb)\n",
    "        new_predictions.append(pred)\n",
    "    final_new_predictions = np.mean(new_predictions, axis=0)\n",
    "    submit['Book-Rating'] = final_new_predictions  # You will have to define `submit` dataframe\n",
    "\n",
    "    return submit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 고도화해줄래? 물어봤더니만\n",
    "#  모든 단계를 더욱 모듈화하고 재사용 가능한 함수를 도입하여 공통 기능을 재활용함으로써 코드의 가독성과 유지 보수성을 향상시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports and functions  ### 혼동으로 인해 코드 수정 안함 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from create_feature_matrix import create_feature_matrix\n",
    "import os\n",
    "from train_model import train\n",
    "from typing import Tuple\n",
    "\n",
    "# Function to prepare node features\n",
    "def prepare_node_features(df: pd.DataFrame, age_tensor: torch.Tensor, user_ids: np.ndarray, book_ids: np.ndarray) -> Tuple[np.ndarray, StandardScaler]:\n",
    "    le = LabelEncoder()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    df['Location_encoded'] = le.fit_transform(df['Location'])\n",
    "    embedding_layer = Embedding(num_embeddings=151, embedding_dim=79)\n",
    "    location_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(df['Location_encoded'].values, dtype=torch.long), dim=1))\n",
    "    location_embeddings = location_embeddings.detach().numpy().squeeze()\n",
    "\n",
    "    pca = PCA(n_components=50)\n",
    "    title_embeddings = df['Book-Title'].apply(get_title_embedding_fasttext).tolist()\n",
    "    title_embeddings_array = np.array(title_embeddings)\n",
    "    reduced_title_embeddings = pca.fit_transform(title_embeddings_array)\n",
    "\n",
    "    df['Publisher_encoded'] = le.fit_transform(df['Publisher'])\n",
    "    embedding_layer = Embedding(num_embeddings=3689, embedding_dim=30)\n",
    "    publisher_embeddings = embedding_layer(torch.unsqueeze(torch.tensor(df['Publisher_encoded'].values, dtype=torch.long), dim=1))\n",
    "    publisher_embeddings = publisher_embeddings.detach().numpy().squeeze()\n",
    "        \n",
    "    age_tensor_scaled = scaler.fit_transform(age_tensor.reshape(-1, 1))\n",
    "    location_embeddings_scaled = scaler.fit_transform(location_embeddings)\n",
    "\n",
    "    feature_matrix = np.zeros((len(user_ids) + len(book_ids), age_tensor_scaled.shape[1] + location_embeddings_scaled.shape[1]))\n",
    "    for user_id, age, location in zip(user_ids, age_tensor_scaled, location_embeddings_scaled):\n",
    "        feature_matrix[user_id] = np.concatenate([age, location], axis=0)\n",
    "\n",
    "    title_embeddings_scaled = scaler.fit_transform(reduced_title_embeddings)\n",
    "    publisher_embeddings_scaled = scaler.fit_transform(publisher_embeddings)\n",
    "    for book_id, title, publisher in zip(book_ids, title_embeddings_scaled, publisher_embeddings_scaled):\n",
    "        feature_matrix[book_id] = np.concatenate([title, publisher], axis=0)\n",
    "\n",
    "    return feature_matrix, scaler\n",
    "\n",
    "# Function to perform hyperparameter optimization for LGBM\n",
    "def perform_hyperopt(train_data):\n",
    "    space = {\n",
    "        'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': hp.quniform('bagging_freq', 1, 7, 1),\n",
    "        'max_depth': hp.quniform('max_depth', 5, 10, 1),\n",
    "        'force_col_wise': hp.choice('force_col_wise', [True]),\n",
    "    }\n",
    "\n",
    "    def objective(params):\n",
    "        params = {\n",
    "            'device': 'gpu',\n",
    "            'num_leaves': int(params['num_leaves']),\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'feature_fraction': params['feature_fraction'],\n",
    "            'bagging_fraction': params['bagging_fraction'],\n",
    "            'bagging_freq': int(params['bagging_freq']),\n",
    "            'max_depth': int(params['max_depth']),\n",
    "            'force_col_wise': params['force_col_wise'],\n",
    "            'objective': 'regression',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'verbose': 0\n",
    "        }\n",
    "        cv_results = lgb.cv(params, train_data, num_boost_round=500, nfold=5, \n",
    "                            early_stopping_rounds=50, metrics='rmse', seed=42)\n",
    "        # Hyperopt will try to minimize loss (it always minimizes the objective)\n",
    "        loss = min(cv_results['rmse-mean'])\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "    sorted_trials = sorted(trials.results, key=lambda x: x['loss'])\n",
    "    return sorted_trials\n",
    "\n",
    "# Function to train models\n",
    "def train_models(sorted_trials, train_data, test_data, X_test, y_test, save_dir='./'):\n",
    "    hyperparameters = [trial['params'] for trial in sorted_trials[:5]]\n",
    "    models = {}\n",
    "    for i, params in enumerate(hyperparameters):\n",
    "        gbm = lgb.train(params, train_data, num_boost_round=500, valid_sets=test_data,\n",
    "                    early_stopping_rounds=10, verbose_eval=False)\n",
    "        y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        models[i] = gbm\n",
    "        gbm.save_model(os.path.join(save_dir, f'model_{i}.txt'))\n",
    "    return models\n",
    "\n",
    "# ... Rest of your code (e.g., model definition, training functions etc.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 검증을 따로 하고, 다른 chat에게 한번 더 검토를 부탁해야 함."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
